#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç¬”è®°æ•°æ®åº“ç®¡ç†ç³»ç»Ÿ
ç®¡ç†çˆ¬å–çš„ç¬”è®°å’Œè¯„è®ºæ•°æ®ï¼Œæ”¯æŒåŠ¨æ€æŸ¥è¯¢å’Œç®¡ç†
"""

import sqlite3
import json
import hashlib
from typing import List, Dict, Optional, Tuple
from datetime import datetime
from loguru import logger
from contextlib import contextmanager


class NoteManager:
    """ç¬”è®°æ•°æ®åº“ç®¡ç†å™¨"""
    
    def __init__(self, db_path: str = "datas/notes.db"):
        self.db_path = db_path
        self._init_database()
        logger.info(f"âœ… ç¬”è®°æ•°æ®åº“å·²è¿æ¥: {db_path}")
    
    @contextmanager
    def _get_connection(self):
        """è·å–æ•°æ®åº“è¿æ¥ï¼ˆä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼‰"""
        conn = sqlite3.connect(self.db_path)
        conn.row_factory = sqlite3.Row
        try:
            yield conn
            conn.commit()
        except Exception as e:
            conn.rollback()
            raise e
        finally:
            conn.close()
    
    def _init_database(self):
        """åˆå§‹åŒ–æ•°æ®åº“è¡¨ç»“æ„"""
        with self._get_connection() as conn:
            cursor = conn.cursor()
            
            # ç¬”è®°ä¸»è¡¨ï¼ˆç²¾ç®€ç‰ˆ - 24ä¸ªæ ¸å¿ƒå­—æ®µï¼‰
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS notes (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    note_id TEXT UNIQUE NOT NULL,
                    url TEXT,
                    
                    -- åŸºæœ¬ä¿¡æ¯
                    title TEXT,
                    desc TEXT,
                    note_type TEXT,
                    
                    -- ä½œè€…ä¿¡æ¯
                    author_id TEXT,
                    author_name TEXT,
                    author_ip_location TEXT,
                    
                    -- äº’åŠ¨æ•°æ®
                    liked_count INTEGER DEFAULT 0,
                    collected_count INTEGER DEFAULT 0,
                    comment_count INTEGER DEFAULT 0,
                    total_interaction INTEGER DEFAULT 0,
                    traffic_level TEXT,
                    
                    -- æ ‡ç­¾ï¼ˆJSONæ•°ç»„ï¼‰
                    tags TEXT,
                    
                    -- æ—¶é—´ä¿¡æ¯
                    upload_time DATETIME,
                    crawl_time DATETIME DEFAULT CURRENT_TIMESTAMP,
                    
                    -- å…³é”®è¯æ¥æºä¸åˆ†ç±»
                    keyword_source TEXT,
                    matrix_category TEXT,
                    city_category TEXT,
                    domain_category TEXT,
                    
                    -- RAGç›¸å…³
                    full_text TEXT,  -- ç”¨äºRAGçš„å®Œæ•´æ–‡æœ¬ï¼ˆæ ‡é¢˜+å†…å®¹+æ ‡ç­¾ï¼‰
                    comments_text TEXT,  -- èšåˆçš„è¯„è®ºæ–‡æœ¬ï¼ˆåªä¿ç•™å†…å®¹ï¼‰
                    vectorized_at DATETIME,  -- å‘é‡åŒ–æ—¶é—´æˆ³ï¼ˆç”¨äºå¢é‡æ›´æ–°ï¼‰
                    
                    -- çˆ¬å–æ‰¹æ¬¡
                    crawl_batch TEXT
                )
            """)
            
            # è¯„è®ºè¡¨
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS comments (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    comment_id TEXT UNIQUE,
                    note_id TEXT NOT NULL,
                    content TEXT NOT NULL,  -- åªä¿ç•™è¯„è®ºå†…å®¹
                    crawl_time DATETIME DEFAULT CURRENT_TIMESTAMP,
                    FOREIGN KEY (note_id) REFERENCES notes(note_id)
                )
            """)
            
            # çˆ¬å–æ‰¹æ¬¡è®°å½•è¡¨
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS crawl_batches (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    batch_id TEXT UNIQUE NOT NULL,
                    keyword TEXT,
                    mode TEXT,
                    notes_count INTEGER DEFAULT 0,
                    comments_count INTEGER DEFAULT 0,
                    start_time DATETIME,
                    end_time DATETIME,
                    status TEXT DEFAULT 'running',
                    created_at DATETIME DEFAULT CURRENT_TIMESTAMP
                )
            """)
            
            # åˆ›å»ºç´¢å¼•
            cursor.execute("CREATE INDEX IF NOT EXISTS idx_notes_note_id ON notes(note_id)")
            cursor.execute("CREATE INDEX IF NOT EXISTS idx_notes_keyword ON notes(keyword_source)")
            cursor.execute("CREATE INDEX IF NOT EXISTS idx_notes_traffic ON notes(traffic_level)")
            cursor.execute("CREATE INDEX IF NOT EXISTS idx_notes_city ON notes(city_category)")
            cursor.execute("CREATE INDEX IF NOT EXISTS idx_notes_domain ON notes(domain_category)")
            cursor.execute("CREATE INDEX IF NOT EXISTS idx_notes_crawl_time ON notes(crawl_time)")
            cursor.execute("CREATE INDEX IF NOT EXISTS idx_comments_note_id ON comments(note_id)")
    
    def add_note(self, note_info: Dict, batch_id: str = None) -> bool:
        """
        æ·»åŠ ç¬”è®°åˆ°æ•°æ®åº“
        
        Args:
            note_info: ç¬”è®°ä¿¡æ¯å­—å…¸
            batch_id: çˆ¬å–æ‰¹æ¬¡ID
        
        Returns:
            æ˜¯å¦æ·»åŠ æˆåŠŸ
        """
        try:
            with self._get_connection() as conn:
                cursor = conn.cursor()
                
                # å¤„ç†tagsä¸ºJSONå­—ç¬¦ä¸²
                tags = note_info.get('tags', [])
                if isinstance(tags, list):
                    tags_json = json.dumps(tags, ensure_ascii=False)
                else:
                    tags_json = str(tags)
                
                # æ„å»ºfull_textç”¨äºRAG
                full_text = self._build_full_text(note_info)
                
                # ç²¾ç®€ç‰ˆINSERTï¼ˆ22ä¸ªå­—æ®µ - ç§»é™¤ author_ip_locationï¼‰
                cursor.execute("""
                    INSERT OR REPLACE INTO notes (
                        note_id, url,
                        title, desc, note_type,
                        author_id, author_name,
                        liked_count, collected_count, comment_count,
                        total_interaction, traffic_level,
                        tags,
                        upload_time, crawl_time,
                        keyword_source, matrix_category, city_category, domain_category,
                        full_text, comments_text,
                        crawl_batch
                    ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                """, (
                    note_info.get('note_id', ''),
                    note_info.get('url', ''),
                    note_info.get('title', ''),
                    note_info.get('desc', ''),
                    note_info.get('note_type', ''),
                    note_info.get('author_id', ''),
                    note_info.get('author_name', ''),
                    self._safe_int(note_info.get('liked_count', 0)),
                    self._safe_int(note_info.get('collected_count', 0)),
                    self._safe_int(note_info.get('comment_count', 0)),
                    self._safe_int(note_info.get('total_interaction', 0)),
                    note_info.get('traffic_level', ''),
                    tags_json,
                    note_info.get('upload_time', ''),
                    datetime.now().isoformat(),
                    note_info.get('keyword_source', ''),
                    note_info.get('matrix_category', ''),
                    note_info.get('city_category', ''),
                    note_info.get('domain_category', ''),
                    full_text,
                    note_info.get('comments_text', ''),
                    batch_id
                ))
                
                logger.debug(f"âœ… æ·»åŠ ç¬”è®°: {note_info.get('title', '')[:30]}...")
                return True
                
        except Exception as e:
            logger.error(f"âŒ æ·»åŠ ç¬”è®°å¤±è´¥: {e}")
            return False
    
    def add_comments(self, note_id: str, comments: List[Dict]) -> int:
        """
        æ‰¹é‡æ·»åŠ è¯„è®ºï¼ˆåªä¿ç•™contentï¼‰
        
        Args:
            note_id: ç¬”è®°ID
            comments: è¯„è®ºåˆ—è¡¨
        
        Returns:
            æ·»åŠ æˆåŠŸçš„è¯„è®ºæ•°é‡
        """
        added_count = 0
        
        with self._get_connection() as conn:
            cursor = conn.cursor()
            
            for idx, comment in enumerate(comments):
                try:
                    content = comment.get('content', '')
                    if not content:
                        continue
                    
                    comment_id = comment.get('comment_id', comment.get('id', ''))
                    # å®‰å…¨é˜²æŠ¤ï¼šå¦‚æœ comment_id ä¸ºç©ºï¼Œè‡ªåŠ¨ç”Ÿæˆå”¯ä¸€ID
                    if not comment_id:
                        content_hash = hashlib.md5(f"{note_id}_{content}_{idx}".encode()).hexdigest()[:12]
                        comment_id = f"{note_id}_{content_hash}"
                    
                    cursor.execute("""
                        INSERT OR IGNORE INTO comments (comment_id, note_id, content)
                        VALUES (?, ?, ?)
                    """, (comment_id, note_id, content))
                    
                    if cursor.rowcount > 0:
                        added_count += 1
                        
                except Exception as e:
                    logger.debug(f"æ·»åŠ è¯„è®ºå¤±è´¥: {e}")
                    continue
        
        return added_count
    
    def get_comments_text_by_note(self, note_id: str) -> str:
        """è·å–ç¬”è®°çš„æ‰€æœ‰è¯„è®ºå†…å®¹ï¼ˆèšåˆä¸ºæ–‡æœ¬ï¼‰"""
        with self._get_connection() as conn:
            cursor = conn.cursor()
            cursor.execute("""
                SELECT content FROM comments WHERE note_id = ?
            """, (note_id,))
            
            contents = [row['content'] for row in cursor.fetchall()]
            return '\n'.join(contents)
    
    def update_comments_text(self, note_id: str):
        """å°†è¯„è®ºå†…å®¹èšåˆåå›å†™åˆ°notesè¡¨çš„comments_textå­—æ®µ"""
        try:
            comments_text = self.get_comments_text_by_note(note_id)
            if comments_text:
                with self._get_connection() as conn:
                    cursor = conn.cursor()
                    cursor.execute("""
                        UPDATE notes SET comments_text = ? WHERE note_id = ?
                    """, (comments_text, note_id))
                    logger.debug(f"âœ… å›å†™è¯„è®ºæ–‡æœ¬: {note_id} ({len(comments_text)}å­—)")
        except Exception as e:
            logger.error(f"âŒ å›å†™è¯„è®ºæ–‡æœ¬å¤±è´¥: {note_id}, {e}")
    
    def _build_full_text(self, note_info: Dict) -> str:
        """æ„å»ºç”¨äºRAGçš„å®Œæ•´æ–‡æœ¬"""
        parts = []
        
        title = note_info.get('title', '')
        if title:
            parts.append(f"æ ‡é¢˜: {title}")
        
        desc = note_info.get('desc', '')
        if desc:
            parts.append(f"å†…å®¹: {desc}")
        
        tags = note_info.get('tags', [])
        if tags:
            if isinstance(tags, list):
                parts.append(f"æ ‡ç­¾: {', '.join(str(t) for t in tags)}")
            else:
                parts.append(f"æ ‡ç­¾: {tags}")
        
        return '\n\n'.join(parts)
    
    def _safe_int(self, value, default=0) -> int:
        """å®‰å…¨è½¬æ¢ä¸ºæ•´æ•°"""
        try:
            if isinstance(value, int):
                return value
            if isinstance(value, str):
                value = value.strip()
                if 'ä¸‡' in value:
                    return int(float(value.replace('ä¸‡', '')) * 10000)
                elif 'w' in value.lower():
                    return int(float(value.replace('w', '').replace('W', '')) * 10000)
                else:
                    return int(value)
            return int(value)
        except:
            return default
    
    # ==================== æŸ¥è¯¢æ–¹æ³• ====================
    
    def get_note_by_id(self, note_id: str) -> Optional[Dict]:
        """æ ¹æ®IDè·å–ç¬”è®°"""
        with self._get_connection() as conn:
            cursor = conn.cursor()
            cursor.execute("SELECT * FROM notes WHERE note_id = ?", (note_id,))
            row = cursor.fetchone()
            return dict(row) if row else None
    
    def note_exists(self, note_id: str) -> bool:
        """æ£€æŸ¥ç¬”è®°æ˜¯å¦å­˜åœ¨"""
        return self.get_note_by_id(note_id) is not None
    
    def get_notes(
        self,
        keyword: Optional[str] = None,
        city: Optional[str] = None,
        domain: Optional[str] = None,
        traffic_level: Optional[str] = None,
        min_interaction: Optional[int] = None,
        limit: Optional[int] = None,
        offset: int = 0,
        order_by: str = 'crawl_time DESC'
    ) -> List[Dict]:
        """
        æŸ¥è¯¢ç¬”è®°
        
        Args:
            keyword: å…³é”®è¯æ¥æºç­›é€‰
            city: åŸå¸‚åˆ†ç±»ç­›é€‰
            domain: é¢†åŸŸåˆ†ç±»ç­›é€‰
            traffic_level: æµé‡å±‚çº§ç­›é€‰
            min_interaction: æœ€å°äº’åŠ¨é‡
            limit: è¿”å›æ•°é‡é™åˆ¶
            offset: åç§»é‡
            order_by: æ’åºæ–¹å¼
        """
        with self._get_connection() as conn:
            cursor = conn.cursor()
            
            query = "SELECT * FROM notes WHERE 1=1"
            params = []
            
            if keyword:
                query += " AND keyword_source = ?"
                params.append(keyword)
            
            if city:
                query += " AND city_category = ?"
                params.append(city)
            
            if domain:
                query += " AND domain_category = ?"
                params.append(domain)
            
            if traffic_level:
                query += " AND traffic_level = ?"
                params.append(traffic_level)
            
            if min_interaction is not None:
                query += " AND total_interaction >= ?"
                params.append(min_interaction)
            
            query += f" ORDER BY {order_by}"
            
            if limit:
                query += " LIMIT ? OFFSET ?"
                params.extend([limit, offset])
            
            cursor.execute(query, params)
            return [dict(row) for row in cursor.fetchall()]
    
    def get_notes_for_rag(self, limit: Optional[int] = None) -> List[Dict]:
        """è·å–ç”¨äºRAGçš„ç¬”è®°æ•°æ®ï¼ˆfull_text + comments_text åˆå¹¶ä¸ºå®Œæ•´æ–‡æœ¬ï¼‰"""
        with self._get_connection() as conn:
            cursor = conn.cursor()
            
            query = """
                SELECT 
                    note_id, title, desc, full_text, comments_text,
                    tags, keyword_source, city_category, domain_category,
                    traffic_level, total_interaction, author_name, upload_time
                FROM notes
                WHERE full_text IS NOT NULL AND full_text != ''
                ORDER BY total_interaction DESC
            """
            
            if limit:
                query += f" LIMIT {limit}"
            
            cursor.execute(query)
            return [dict(row) for row in cursor.fetchall()]
    
    def get_unvectorized_notes(self, limit: Optional[int] = None) -> List[Dict]:
        """è·å–æœªå‘é‡åŒ–çš„ç¬”è®°ï¼ˆç”¨äºå¢é‡æ›´æ–°ï¼‰"""
        with self._get_connection() as conn:
            cursor = conn.cursor()
            
            query = """
                SELECT 
                    note_id, title, desc, full_text, comments_text,
                    tags, keyword_source, city_category, domain_category,
                    traffic_level, total_interaction, author_name, upload_time
                FROM notes
                WHERE full_text IS NOT NULL AND full_text != ''
                  AND (vectorized_at IS NULL OR vectorized_at = '')
                ORDER BY total_interaction DESC
            """
            
            if limit:
                query += f" LIMIT {limit}"
            
            cursor.execute(query)
            return [dict(row) for row in cursor.fetchall()]
    
    def mark_as_vectorized(self, note_ids: List[str]):
        """æ ‡è®°ç¬”è®°ä¸ºå·²å‘é‡åŒ–"""
        if not note_ids:
            return
        
        with self._get_connection() as conn:
            cursor = conn.cursor()
            timestamp = datetime.now().isoformat()
            
            placeholders = ','.join('?' * len(note_ids))
            cursor.execute(f"""
                UPDATE notes 
                SET vectorized_at = ?
                WHERE note_id IN ({placeholders})
            """, [timestamp] + note_ids)
            
            logger.info(f"âœ… æ ‡è®° {len(note_ids)} æ¡ç¬”è®°ä¸ºå·²å‘é‡åŒ–")
    
    def search_notes(self, query: str, limit: int = 50) -> List[Dict]:
        """å…¨æ–‡æœç´¢ç¬”è®°"""
        with self._get_connection() as conn:
            cursor = conn.cursor()
            
            # SQLite LIKEæœç´¢
            search_pattern = f"%{query}%"
            cursor.execute("""
                SELECT * FROM notes
                WHERE title LIKE ? OR desc LIKE ? OR full_text LIKE ?
                ORDER BY total_interaction DESC
                LIMIT ?
            """, (search_pattern, search_pattern, search_pattern, limit))
            
            return [dict(row) for row in cursor.fetchall()]
    
    # ==================== æ‰¹æ¬¡ç®¡ç† ====================
    
    def start_batch(self, keyword: str = None, mode: str = None) -> str:
        """å¼€å§‹æ–°çš„çˆ¬å–æ‰¹æ¬¡"""
        batch_id = f"batch_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        with self._get_connection() as conn:
            cursor = conn.cursor()
            cursor.execute("""
                INSERT INTO crawl_batches (batch_id, keyword, mode, start_time, status)
                VALUES (?, ?, ?, ?, 'running')
            """, (batch_id, keyword, mode, datetime.now().isoformat()))
        
        logger.info(f"ğŸ“¦ å¼€å§‹æ‰¹æ¬¡: {batch_id}")
        return batch_id
    
    def finish_batch(self, batch_id: str, notes_count: int, comments_count: int):
        """å®Œæˆçˆ¬å–æ‰¹æ¬¡"""
        with self._get_connection() as conn:
            cursor = conn.cursor()
            cursor.execute("""
                UPDATE crawl_batches 
                SET end_time = ?, notes_count = ?, comments_count = ?, status = 'completed'
                WHERE batch_id = ?
            """, (datetime.now().isoformat(), notes_count, comments_count, batch_id))
        
        logger.info(f"âœ… å®Œæˆæ‰¹æ¬¡: {batch_id} (ç¬”è®°:{notes_count}, è¯„è®º:{comments_count})")
    
    # ==================== ç»Ÿè®¡æ–¹æ³• ====================
    
    def get_statistics(self) -> Dict:
        """è·å–æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯"""
        with self._get_connection() as conn:
            cursor = conn.cursor()
            
            # ç¬”è®°æ€»æ•°
            cursor.execute("SELECT COUNT(*) as total FROM notes")
            total_notes = cursor.fetchone()['total']
            
            # è¯„è®ºæ€»æ•°
            cursor.execute("SELECT COUNT(*) as total FROM comments")
            total_comments = cursor.fetchone()['total']
            
            # æŒ‰åŸå¸‚ç»Ÿè®¡
            cursor.execute("""
                SELECT city_category, COUNT(*) as count 
                FROM notes 
                WHERE city_category IS NOT NULL AND city_category != ''
                GROUP BY city_category
            """)
            by_city = {row['city_category']: row['count'] for row in cursor.fetchall()}
            
            # æŒ‰é¢†åŸŸç»Ÿè®¡
            cursor.execute("""
                SELECT domain_category, COUNT(*) as count 
                FROM notes 
                WHERE domain_category IS NOT NULL AND domain_category != ''
                GROUP BY domain_category
            """)
            by_domain = {row['domain_category']: row['count'] for row in cursor.fetchall()}
            
            # æŒ‰æµé‡å±‚çº§ç»Ÿè®¡
            cursor.execute("""
                SELECT traffic_level, COUNT(*) as count 
                FROM notes 
                WHERE traffic_level IS NOT NULL AND traffic_level != ''
                GROUP BY traffic_level
            """)
            by_traffic = {row['traffic_level']: row['count'] for row in cursor.fetchall()}
            
            # æŒ‰å…³é”®è¯ç»Ÿè®¡ï¼ˆå‰20ï¼‰
            cursor.execute("""
                SELECT keyword_source, COUNT(*) as count 
                FROM notes 
                WHERE keyword_source IS NOT NULL AND keyword_source != ''
                GROUP BY keyword_source
                ORDER BY count DESC
                LIMIT 20
            """)
            top_keywords = {row['keyword_source']: row['count'] for row in cursor.fetchall()}
            
            # æ‰¹æ¬¡ç»Ÿè®¡
            cursor.execute("SELECT COUNT(*) as total FROM crawl_batches WHERE status = 'completed'")
            completed_batches = cursor.fetchone()['total']
            
            return {
                'total_notes': total_notes,
                'total_comments': total_comments,
                'completed_batches': completed_batches,
                'by_city': by_city,
                'by_domain': by_domain,
                'by_traffic_level': by_traffic,
                'top_keywords': top_keywords
            }
    
    # ==================== å¯¼å‡ºæ–¹æ³• ====================
    
    def export_to_jsonl(self, output_path: str, limit: Optional[int] = None, incremental: bool = False) -> int:
        """å¯¼å‡ºä¸ºJSONLæ ¼å¼ï¼ˆé€‚åˆRAGï¼‰"""
        import os
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        
        # å¢é‡æ¨¡å¼åªå¯¼å‡ºæœªå‘é‡åŒ–çš„ç¬”è®°
        if incremental:
            notes = self.get_unvectorized_notes(limit=limit)
            logger.info(f"ğŸ“ å¢é‡æ¨¡å¼ï¼šå¯¼å‡º {len(notes)} æ¡æœªå‘é‡åŒ–ç¬”è®°")
        else:
            notes = self.get_notes_for_rag(limit=limit)
        
        with open(output_path, 'w', encoding='utf-8') as f:
            for note in notes:
                # æ„å»ºRAGæ–‡æ¡£ - å°†è¯„è®ºåˆå¹¶åˆ°ä¸»æ–‡æœ¬ä¸­ï¼Œæå‡æ£€ç´¢å‘½ä¸­ç‡
                rag_text = note['full_text'] or ''
                comments_text = note.get('comments_text', '') or ''
                if comments_text:
                    rag_text += f'\n\nç”¨æˆ·è¯„è®º: {comments_text}'
                
                doc = {
                    'id': note['note_id'],
                    'text': rag_text,
                    'metadata': {
                        'title': note['title'],
                        'author': note['author_name'],
                        'keyword': note['keyword_source'],
                        'city': note['city_category'],
                        'domain': note['domain_category'],
                        'traffic_level': note['traffic_level'],
                        'interaction': note['total_interaction'],
                        'upload_time': note['upload_time']
                    }
                }
                
                f.write(json.dumps(doc, ensure_ascii=False) + '\n')
        
        # å¦‚æœæ˜¯å¢é‡æ¨¡å¼ï¼Œæ ‡è®°ä¸ºå·²å‘é‡åŒ–
        if incremental and notes:
            note_ids = [note['note_id'] for note in notes]
            self.mark_as_vectorized(note_ids)
        
        logger.info(f"âœ… å¯¼å‡º {len(notes)} æ¡ç¬”è®°åˆ° {output_path}")
        return len(notes)
    
    def export_comments_only(self, output_path: str) -> int:
        """åªå¯¼å‡ºè¯„è®ºå†…å®¹"""
        import os
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        
        with self._get_connection() as conn:
            cursor = conn.cursor()
            cursor.execute("SELECT note_id, content FROM comments")
            comments = cursor.fetchall()
        
        with open(output_path, 'w', encoding='utf-8') as f:
            for comment in comments:
                f.write(json.dumps({
                    'note_id': comment['note_id'],
                    'content': comment['content']
                }, ensure_ascii=False) + '\n')
        
        logger.info(f"âœ… å¯¼å‡º {len(comments)} æ¡è¯„è®ºåˆ° {output_path}")
        return len(comments)


# ä¾¿æ·å‡½æ•°
def get_note_manager(db_path: str = "datas/notes.db") -> NoteManager:
    """è·å–NoteManagerå®ä¾‹"""
    return NoteManager(db_path)
