#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¤šæ ¼å¼æ•°æ®å­˜å‚¨ç®¡ç†å™¨
æ”¯æŒ CSVã€JSONã€Excelã€SQLite å››ç§å­˜å‚¨æ–¹å¼
"""

import csv
import json
import os
import sqlite3
from datetime import datetime
from typing import List, Dict, Optional
from pathlib import Path
from loguru import logger


class StorageManager:
    """ç»Ÿä¸€çš„æ•°æ®å­˜å‚¨ç®¡ç†å™¨"""
    
    def __init__(self, storage_type: str = "sqlite", output_dir: str = "datas"):
        """
        åˆå§‹åŒ–å­˜å‚¨ç®¡ç†å™¨
        
        Args:
            storage_type: å­˜å‚¨ç±»å‹ ('csv', 'json', 'excel', 'sqlite')
            output_dir: è¾“å‡ºç›®å½•ï¼ˆå°†è‡ªåŠ¨åœ¨å…¶ä¸‹åˆ›å»ºæ ¼å¼ä¸“ç”¨å­ç›®å½•ï¼‰
        """
        self.storage_type = storage_type.lower()
        
        # æ ¹æ®å­˜å‚¨æ ¼å¼åˆ›å»ºä¸“ç”¨å­ç›®å½•
        format_dirs = {
            'sqlite': 'sqlite_datas',
            'csv': 'csv_datas',
            'json': 'json_datas',
            'excel': 'excel_datas'
        }
        
        base_dir = Path(output_dir)
        self.output_dir = base_dir / format_dirs.get(self.storage_type, self.storage_type)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # å­˜å‚¨æ–‡ä»¶/æ•°æ®åº“è·¯å¾„
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.notes_data = []
        self.comments_data = []
        
        if self.storage_type == "sqlite":
            self.db_path = self.output_dir / "notes.db"
            self._init_sqlite()
        elif self.storage_type == "csv":
            self.notes_file = self.output_dir / f"notes_{timestamp}.csv"
            self.comments_file = self.output_dir / f"comments_{timestamp}.csv"
            self._init_csv()
        elif self.storage_type == "json":
            self.json_file = self.output_dir / f"notes_{timestamp}.json"
        elif self.storage_type == "excel":
            self.excel_file = self.output_dir / f"notes_{timestamp}.xlsx"
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„å­˜å‚¨ç±»å‹: {storage_type}")
        
        logger.info(f"âœ… å­˜å‚¨ç®¡ç†å™¨å·²åˆå§‹åŒ–: {storage_type.upper()}")
    
    def _init_sqlite(self):
        """åˆå§‹åŒ– SQLite æ•°æ®åº“"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # åˆ›å»ºç¬”è®°è¡¨
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS notes (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                note_id TEXT UNIQUE NOT NULL,
                url TEXT,
                title TEXT,
                desc TEXT,
                note_type TEXT,
                author_id TEXT,
                author_name TEXT,
                liked_count INTEGER DEFAULT 0,
                collected_count INTEGER DEFAULT 0,
                comment_count INTEGER DEFAULT 0,
                total_interaction INTEGER DEFAULT 0,
                traffic_level TEXT,
                tags TEXT,
                upload_time DATETIME,
                crawl_time DATETIME DEFAULT CURRENT_TIMESTAMP,
                keyword_source TEXT,
                full_text TEXT
            )
        """)
        
        # åˆ›å»ºè¯„è®ºè¡¨
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS comments (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                comment_id TEXT UNIQUE,
                note_id TEXT NOT NULL,
                content TEXT NOT NULL,
                author_name TEXT,
                like_count INTEGER DEFAULT 0,
                crawl_time DATETIME DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (note_id) REFERENCES notes(note_id)
            )
        """)
        
        conn.commit()
        conn.close()
    
    def _init_csv(self):
        """åˆå§‹åŒ– CSV æ–‡ä»¶ï¼ˆå†™å…¥è¡¨å¤´ï¼‰"""
        # ç¬”è®° CSV
        with open(self.notes_file, 'w', newline='', encoding='utf-8-sig') as f:
            writer = csv.DictWriter(f, fieldnames=[
                'note_id', 'url', 'title', 'desc', 'note_type',
                'author_id', 'author_name', 'liked_count', 'collected_count',
                'comment_count', 'total_interaction', 'traffic_level',
                'tags', 'upload_time', 'crawl_time', 'keyword_source', 'full_text'
            ])
            writer.writeheader()
        
        # è¯„è®º CSV
        with open(self.comments_file, 'w', newline='', encoding='utf-8-sig') as f:
            writer = csv.DictWriter(f, fieldnames=[
                'comment_id', 'note_id', 'content', 'author_name', 'like_count', 'crawl_time'
            ])
            writer.writeheader()
    
    def add_note(self, note: Dict):
        """æ·»åŠ ç¬”è®°æ•°æ®"""
        if self.storage_type == "sqlite":
            self._add_note_sqlite(note)
        elif self.storage_type == "csv":
            self._add_note_csv(note)
        else:
            # JSON å’Œ Excel å…ˆå­˜åˆ°å†…å­˜
            self.notes_data.append(note)
    
    def _add_note_sqlite(self, note: Dict):
        """æ·»åŠ ç¬”è®°åˆ° SQLite"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            cursor.execute("""
                INSERT OR REPLACE INTO notes (
                    note_id, url, title, desc, note_type,
                    author_id, author_name, liked_count, collected_count,
                    comment_count, total_interaction, traffic_level,
                    tags, upload_time, keyword_source, full_text
                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            """, (
                note.get('note_id'), note.get('url'), note.get('title'),
                note.get('desc'), note.get('note_type', 'normal'),
                note.get('author_id'), note.get('author_name'),
                note.get('liked_count', 0), note.get('collected_count', 0),
                note.get('comment_count', 0), note.get('total_interaction', 0),
                note.get('traffic_level', ''),
                json.dumps(note.get('tags', []), ensure_ascii=False),
                note.get('upload_time'), note.get('keyword_source', ''),
                note.get('full_text', '')
            ))
            
            conn.commit()
            conn.close()
        except Exception as e:
            logger.error(f"SQLite å†™å…¥å¤±è´¥: {e}")
    
    def _add_note_csv(self, note: Dict):
        """æ·»åŠ ç¬”è®°åˆ° CSV"""
        try:
            with open(self.notes_file, 'a', newline='', encoding='utf-8-sig') as f:
                writer = csv.DictWriter(f, fieldnames=[
                    'note_id', 'url', 'title', 'desc', 'note_type',
                    'author_id', 'author_name', 'liked_count', 'collected_count',
                    'comment_count', 'total_interaction', 'traffic_level',
                    'tags', 'upload_time', 'crawl_time', 'keyword_source', 'full_text'
                ])
                
                row = {
                    'note_id': note.get('note_id'),
                    'url': note.get('url'),
                    'title': note.get('title'),
                    'desc': note.get('desc'),
                    'note_type': note.get('note_type', 'normal'),
                    'author_id': note.get('author_id'),
                    'author_name': note.get('author_name'),
                    'liked_count': note.get('liked_count', 0),
                    'collected_count': note.get('collected_count', 0),
                    'comment_count': note.get('comment_count', 0),
                    'total_interaction': note.get('total_interaction', 0),
                    'traffic_level': note.get('traffic_level', ''),
                    'tags': '|'.join(note.get('tags', [])),
                    'upload_time': note.get('upload_time'),
                    'crawl_time': datetime.now().isoformat(),
                    'keyword_source': note.get('keyword_source', ''),
                    'full_text': note.get('full_text', '')
                }
                
                writer.writerow(row)
        except Exception as e:
            logger.error(f"CSV å†™å…¥å¤±è´¥: {e}")
    
    def add_comments(self, note_id: str, comments: List[Dict]):
        """æ·»åŠ è¯„è®ºæ•°æ®"""
        if not comments:
            return
        
        if self.storage_type == "sqlite":
            self._add_comments_sqlite(note_id, comments)
        elif self.storage_type == "csv":
            self._add_comments_csv(note_id, comments)
        else:
            # JSON å’Œ Excel å­˜åˆ°å†…å­˜
            for comment in comments:
                comment['note_id'] = note_id
                self.comments_data.append(comment)
    
    def _add_comments_sqlite(self, note_id: str, comments: List[Dict]):
        """æ·»åŠ è¯„è®ºåˆ° SQLite"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            for comment in comments:
                cursor.execute("""
                    INSERT OR REPLACE INTO comments (
                        comment_id, note_id, content, author_name, like_count
                    ) VALUES (?, ?, ?, ?, ?)
                """, (
                    comment.get('comment_id'),
                    note_id,
                    comment.get('content', ''),
                    comment.get('author_name', ''),
                    comment.get('like_count', 0)
                ))
            
            conn.commit()
            conn.close()
        except Exception as e:
            logger.error(f"SQLite è¯„è®ºå†™å…¥å¤±è´¥: {e}")
    
    def _add_comments_csv(self, note_id: str, comments: List[Dict]):
        """æ·»åŠ è¯„è®ºåˆ° CSV"""
        try:
            with open(self.comments_file, 'a', newline='', encoding='utf-8-sig') as f:
                writer = csv.DictWriter(f, fieldnames=[
                    'comment_id', 'note_id', 'content', 'author_name', 'like_count', 'crawl_time'
                ])
                
                for comment in comments:
                    row = {
                        'comment_id': comment.get('comment_id'),
                        'note_id': note_id,
                        'content': comment.get('content', ''),
                        'author_name': comment.get('author_name', ''),
                        'like_count': comment.get('like_count', 0),
                        'crawl_time': datetime.now().isoformat()
                    }
                    writer.writerow(row)
        except Exception as e:
            logger.error(f"CSV è¯„è®ºå†™å…¥å¤±è´¥: {e}")
    
    def finalize(self):
        """å®Œæˆå­˜å‚¨ï¼ˆç”¨äº JSON å’Œ Excel çš„æœ€ç»ˆå†™å…¥ï¼‰"""
        if self.storage_type == "json":
            self._save_json()
        elif self.storage_type == "excel":
            self._save_excel()
    
    def _save_json(self):
        """ä¿å­˜ä¸º JSON æ–‡ä»¶"""
        try:
            data = {
                'notes': self.notes_data,
                'comments': self.comments_data,
                'crawl_time': datetime.now().isoformat(),
                'total_notes': len(self.notes_data),
                'total_comments': len(self.comments_data)
            }
            
            with open(self.json_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            
            logger.info(f"âœ… JSON æ–‡ä»¶å·²ä¿å­˜: {self.json_file}")
        except Exception as e:
            logger.error(f"JSON ä¿å­˜å¤±è´¥: {e}")
    
    def _save_excel(self):
        """ä¿å­˜ä¸º Excel æ–‡ä»¶"""
        try:
            import pandas as pd
            
            # å¤„ç†ç¬”è®°æ•°æ®
            notes_df = pd.DataFrame(self.notes_data)
            if 'tags' in notes_df.columns:
                notes_df['tags'] = notes_df['tags'].apply(lambda x: '|'.join(x) if isinstance(x, list) else '')
            
            # å¤„ç†è¯„è®ºæ•°æ®
            comments_df = pd.DataFrame(self.comments_data) if self.comments_data else pd.DataFrame()
            
            # å†™å…¥ Excel
            with pd.ExcelWriter(self.excel_file, engine='openpyxl') as writer:
                notes_df.to_excel(writer, sheet_name='Notes', index=False)
                if not comments_df.empty:
                    comments_df.to_excel(writer, sheet_name='Comments', index=False)
            
            logger.info(f"âœ… Excel æ–‡ä»¶å·²ä¿å­˜: {self.excel_file}")
        except ImportError:
            logger.error("âŒ è¯·å®‰è£… pandas å’Œ openpyxl: pip install pandas openpyxl")
        except Exception as e:
            logger.error(f"Excel ä¿å­˜å¤±è´¥: {e}")
    
    def note_exists(self, note_id: str) -> bool:
        """æ£€æŸ¥ç¬”è®°æ˜¯å¦å­˜åœ¨"""
        if self.storage_type == "sqlite":
            try:
                conn = sqlite3.connect(self.db_path)
                cursor = conn.execute("SELECT 1 FROM notes WHERE note_id = ?", (note_id,))
                exists = cursor.fetchone() is not None
                conn.close()
                return exists
            except (sqlite3.Error, sqlite3.DatabaseError) as e:
                logger.debug(f"æ£€æŸ¥ç¬”è®°æ˜¯å¦å­˜åœ¨å¤±è´¥: {e}")
                return False
        else:
            # å¯¹äºé SQLite æ ¼å¼ï¼Œåªèƒ½æ£€æŸ¥å½“å‰ä¼šè¯å·²ç¼“å­˜çš„æ•°æ®
            # æ³¨æ„ï¼šè¿™æ— æ³•å®ç°è·¨æ¬¡è¿è¡Œçš„å»é‡ï¼ˆé™¤éæ¯æ¬¡éƒ½åŠ è½½æ‰€æœ‰å†å²æ–‡ä»¶ï¼Œé‚£å¤ªæ…¢äº†ï¼‰
            # å¦‚æœéœ€è¦ä¸¥æ ¼çš„è·¨æ¬¡å»é‡ï¼Œè¯·ä½¿ç”¨ sqlite æ ¼å¼
            return any(note.get('note_id') == note_id for note in self.notes_data)

    def get_seen_note_ids(self) -> set:
        """è·å–å·²çˆ¬å–çš„ç¬”è®°IDï¼ˆç”¨äºå»é‡ï¼‰"""
        seen_ids = set()
        
        if self.storage_type == "sqlite":
            try:
                conn = sqlite3.connect(self.db_path)
                cursor = conn.execute("SELECT note_id FROM notes")
                seen_ids = {row[0] for row in cursor if row[0]}
                conn.close()
            except (sqlite3.Error, sqlite3.DatabaseError) as e:
                logger.debug(f"åŠ è½½SQLiteå†å²ç¬”è®°IDå¤±è´¥: {e}")
        elif self.storage_type == "csv":
            try:
                import glob
                csv_pattern = str(self.output_dir / "notes_*.csv")
                csv_files = glob.glob(csv_pattern)
                
                if csv_files:
                    logger.info(f"   ğŸ“‚ æ‰«æåˆ° {len(csv_files)} ä¸ªå†å²CSVæ–‡ä»¶ï¼ŒåŠ è½½ç¬”è®°ID...")
                    for csv_file in csv_files:
                        try:
                            with open(csv_file, 'r', encoding='utf-8-sig') as f:
                                reader = csv.DictReader(f)
                                for row in reader:
                                    if row.get('note_id'):
                                        seen_ids.add(row['note_id'])
                        except (IOError, KeyError) as e:
                            logger.debug(f"è¯»å–CSVæ–‡ä»¶ {csv_file} å¤±è´¥: {e}")
                    
                    logger.info(f"   âœ… å·²åŠ è½½ {len(seen_ids)} ä¸ªå†å²ç¬”è®°IDç”¨äºå»é‡")
            except (FileNotFoundError, IOError, KeyError) as e:
                logger.debug(f"åŠ è½½CSVå†å²ç¬”è®°IDå¤±è´¥: {e}")
        elif self.storage_type == "json":
            try:
                import glob
                json_pattern = str(self.output_dir / "notes_*.json")
                json_files = glob.glob(json_pattern)
                
                if json_files:
                    logger.info(f"   ğŸ“‚ æ‰«æåˆ° {len(json_files)} ä¸ªå†å²JSONæ–‡ä»¶ï¼ŒåŠ è½½ç¬”è®°ID...")
                    for json_file in json_files:
                        try:
                            with open(json_file, 'r', encoding='utf-8') as f:
                                data = json.load(f)
                                for note in data.get('notes', []):
                                    if note.get('note_id'):
                                        seen_ids.add(note['note_id'])
                        except (IOError, json.JSONDecodeError) as e:
                            logger.debug(f"è¯»å–JSONæ–‡ä»¶ {json_file} å¤±è´¥: {e}")
                    
                    logger.info(f"   âœ… å·²åŠ è½½ {len(seen_ids)} ä¸ªå†å²ç¬”è®°IDç”¨äºå»é‡")
            except Exception as e:
                logger.debug(f"åŠ è½½JSONå†å²ç¬”è®°IDå¤±è´¥: {e}")
        else:
            seen_ids = {note.get('note_id') for note in self.notes_data if note.get('note_id')}
        
        return seen_ids
