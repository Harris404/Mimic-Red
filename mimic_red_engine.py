# -*- coding: utf-8 -*-
"""
DrissionPage ç‰ˆå°çº¢ä¹¦æ‰¹é‡çˆ¬è™« (å¤šæ ¼å¼å­˜å‚¨ç‰ˆ)
æ ¸å¿ƒç­–ç•¥ï¼šç§»é™¤æ‰€æœ‰ API ç›‘å¬ (é™ä½ç‰¹å¾) -> çº¯ DOM äº¤äº’ (ç‚¹å‡»/æ»šåŠ¨) -> è¢«åŠ¨ SSR/DOM æå–
æ”¯æŒå­˜å‚¨æ ¼å¼ï¼šCSVã€JSONã€Excelã€SQLite
"""
import sys
import time
import random
import json
import os
import hashlib
import argparse
from datetime import datetime
from typing import List, Dict, Optional

from loguru import logger
from DrissionPage import ChromiumPage, ChromiumOptions

# é…ç½®æ—¥å¿—è½®è½¬
logger.add(
    "logs/spider_{time:YYYY-MM-DD}.log",
    rotation="00:00",  # æ¯å¤©åˆå¤œè½®è½¬
    retention="7 days", # ä¿ç•™7å¤©
    level="INFO",
    encoding="utf-8"
)

# åŠ è½½ CSS é€‰æ‹©å™¨é…ç½®
SELECTORS = {}
try:
    with open('selectors.json', 'r', encoding='utf-8') as f:
        SELECTORS = json.load(f)
except Exception as e:
    logger.warning(f"âš ï¸ æœªæ‰¾åˆ° selectors.json æˆ–åŠ è½½å¤±è´¥ ({e})ï¼Œå°†ä½¿ç”¨é»˜è®¤ç¡¬ç¼–ç é€‰æ‹©å™¨")

# å¯¼å…¥æ–°çš„å­˜å‚¨ç®¡ç†å™¨
try:
    from xhs_utils.storage_manager import StorageManager
except ImportError as e:
    logger.error(f"å¯¼å…¥ä¾èµ–å¤±è´¥: {e}")
    sys.exit(1)

class DataDeduplicator:
    def __init__(self, storage_manager: StorageManager = None):
        self.storage = storage_manager
        self.local_seen = set()
    
    def is_duplicate(self, note_id: str) -> bool:
        # 1. æ£€æŸ¥æœ¬æ¬¡è¿è¡Œçš„å†…å­˜ç¼“å­˜
        if note_id in self.local_seen:
            return True
            
        # 2. æ£€æŸ¥æŒä¹…åŒ–å­˜å‚¨ (SQLite)
        if self.storage and self.storage.note_exists(note_id):
            self.local_seen.add(note_id) # æ›´æ–°æœ¬åœ°ç¼“å­˜
            return True
            
        self.local_seen.add(note_id)
        return False

import re

class DrissionXHSSpider:
    def __init__(self, storage_type: str = "sqlite", output_dir: str = "datas", takeover: bool = True):
        self.storage_type = storage_type
        self.output_dir = output_dir
        self.takeover = takeover
        self.page = None
        self.storage = None
        self.deduplicator = None
        self.stats = {"total_notes": 0, "failed_keywords": 0, "start_time": None}
        
        # åçˆ¬æ§åˆ¶
        self._consecutive_failures = 0
        self._request_count = 0
        self._blocked_count = 0

    def init_browser(self):
        """åˆå§‹åŒ–æµè§ˆå™¨"""
        if not self.takeover:
            logger.error("âŒ æ¨èä½¿ç”¨æ¥ç®¡æ¨¡å¼ (takeover=True) ä»¥é™ä½é£é™©")
            sys.exit(1)
            
        logger.info("ğŸš€ å°è¯•æ¥ç®¡ Chrome (9222)...")
        try:
            self.page = ChromiumPage(addr_or_opts='127.0.0.1:9222')
            current_url = self.page.url or ''
            logger.info(f"   âœ… æ¥ç®¡æˆåŠŸï¼Œå½“å‰é¡µé¢: {current_url[:60]}...")
            
            if 'xiaohongshu.com' not in current_url:
                self.page.get('https://www.xiaohongshu.com')
                time.sleep(2)
        except Exception as e:
            logger.error(f"   âŒ æ¥ç®¡å¤±è´¥: {e}")
            sys.exit(1)
        
        try:
            self.storage = StorageManager(self.storage_type, self.output_dir)
            self.deduplicator = DataDeduplicator(self.storage)
            logger.info(f"   âœ… å­˜å‚¨ç®¡ç†å™¨å·²åˆå§‹åŒ– ({self.storage_type.upper()})")
        except Exception as e:
            logger.error(f"   âŒ å­˜å‚¨ç®¡ç†å™¨åˆå§‹åŒ–å¤±è´¥: {e}")


    def _warmup_session(self):
        """
        ä¼šè¯é¢„çƒ­ï¼šæ¨¡æ‹ŸçœŸå®ç”¨æˆ·å…ˆéšæ„æµè§ˆå†å¼€å§‹çˆ¬å–
        å»ºç«‹æ­£å¸¸çš„è¡Œä¸ºåŸºçº¿ï¼Œé™ä½è¢«é£æ§æ£€æµ‹çš„æ¦‚ç‡
        """
        logger.info("ğŸ¯ ä¼šè¯é¢„çƒ­ä¸­ï¼ˆæ¨¡æ‹Ÿæ­£å¸¸æµè§ˆ 30-60ç§’ï¼‰...")
        try:
            # 1. è®¿é—®é¦–é¡µå¹¶æ»šåŠ¨æµè§ˆ
            self.page.get('https://www.xiaohongshu.com')
            time.sleep(random.uniform(3, 5))
            
            # 2. éšæ„æ»šåŠ¨é¦–é¡µ feed
            for _ in range(random.randint(2, 4)):
                self._random_mouse_move()
                self._human_like_scroll('down', random.randint(300, 600))
                time.sleep(random.uniform(2, 4))
            
            # 3. éšæœºç‚¹å‡»ä¸€ç¯‡æ¨èç¬”è®°çœ‹çœ‹ï¼ˆå»ºç«‹â€œæ­£å¸¸ç”¨æˆ·â€æ¨¡å¼ï¼‰
            try:
                feed_link = self.page.run_js("""
                    const links = document.querySelectorAll('section.note-item a[href*="/explore/"]');
                    if (links.length > 0) {
                        const idx = Math.floor(Math.random() * Math.min(links.length, 5));
                        return links[idx].href;
                    }
                    return null;
                """)
                if feed_link:
                    self.page.get(feed_link)
                    time.sleep(random.uniform(4, 8))  # â€œé˜…è¯»â€å‡ ç§’
                    self._human_like_scroll('down', random.randint(200, 500))
                    time.sleep(random.uniform(2, 4))
            except:
                pass
            
            # 4. å›åˆ°é¦–é¡µ
            self.page.get('https://www.xiaohongshu.com')
            time.sleep(random.uniform(2, 4))
            
            logger.info("âœ… é¢„çƒ­å®Œæˆï¼Œå¼€å§‹çˆ¬å–")
        except Exception as e:
            logger.debug(f"é¢„çƒ­å¼‚å¸¸: {e}")

    def _load_progress(self) -> tuple[set, int]:
        """åŠ è½½å·²å®Œæˆçš„å…³é”®è¯å’Œä»Šæ—¥çˆ¬å–è®¡æ•°ï¼ˆæ”¯æŒæ–­ç‚¹ç»­çˆ¬ï¼‰"""
        progress_file = 'datas/crawl_progress.json'
        if os.path.exists(progress_file):
            try:
                with open(progress_file, 'r') as f:
                    data = json.load(f)
                # åªä¿ç•™å½“å¤©çš„è¿›åº¦
                today = datetime.now().strftime('%Y-%m-%d')
                if data.get('date') == today:
                    return set(data.get('done_keywords', [])), data.get('daily_count', 0)
            except:
                pass
        return set(), 0

    def _save_progress(self, done_keywords: set, daily_count: int):
        """ä¿å­˜çˆ¬å–è¿›åº¦"""
        progress_file = 'datas/crawl_progress.json'
        try:
            with open(progress_file, 'w') as f:
                json.dump({
                    'date': datetime.now().strftime('%Y-%m-%d'),
                    'done_keywords': list(done_keywords),
                    'daily_count': daily_count,
                    'updated_at': datetime.now().isoformat()
                }, f, ensure_ascii=False)
        except:
            pass

    def _random_mouse_move(self):
        """æ¨¡æ‹Ÿäººç±»éšæœºé¼ æ ‡ç§»åŠ¨ï¼ˆè´å¡å°”æ›²çº¿è½¨è¿¹ï¼‰"""
        try:
            # è·å–å½“å‰é¼ æ ‡ä½ç½®ï¼ˆæˆ–éšæœºèµ·ç‚¹ï¼‰
            start_x = random.randint(200, 600)
            start_y = random.randint(200, 500)
            end_x = random.randint(300, 900)
            end_y = random.randint(200, 600)
            
            # æ¨¡æ‹Ÿè´å¡å°”æ›²çº¿ç§»åŠ¨ï¼ˆåˆ†å¤šæ­¥ï¼‰
            steps = random.randint(5, 12)
            for i in range(steps):
                t = i / steps
                # ç®€åŒ–çš„äºŒæ¬¡è´å¡å°”æ›²çº¿
                ctrl_x = (start_x + end_x) / 2 + random.randint(-50, 50)
                ctrl_y = (start_y + end_y) / 2 + random.randint(-80, 80)
                x = int((1-t)**2 * start_x + 2*(1-t)*t * ctrl_x + t**2 * end_x)
                y = int((1-t)**2 * start_y + 2*(1-t)*t * ctrl_y + t**2 * end_y)
                
                try:
                    self.page.actions.move_to((x, y), duration=random.uniform(0.02, 0.08))
                except: pass
                time.sleep(random.uniform(0.01, 0.05))
        except: pass

    def _human_like_scroll(self, direction: str = 'down', distance: int = None):
        """æ¨¡æ‹Ÿäººç±»æ»šåŠ¨è¡Œä¸º"""
        try:
            if distance is None:
                distance = random.randint(300, 600)
            
            # åˆ†å¤šæ¬¡å°æ»šåŠ¨
            scroll_times = random.randint(2, 4)
            per_scroll = distance // scroll_times
            
            for _ in range(scroll_times):
                if direction == 'down':
                    self.page.scroll.down(per_scroll + random.randint(-30, 30))
                else:
                    self.page.scroll.up(per_scroll + random.randint(-30, 30))
                time.sleep(random.uniform(0.1, 0.3))
        except: pass

    def _smart_delay(self, action: str = 'detail'):
        """æ™ºèƒ½å»¶è¿Ÿï¼šæ ¹æ®æ—¶æ®µã€è¯·æ±‚æ¬¡æ•°ã€è¿ç»­å¤±è´¥åŠ¨æ€è°ƒèŠ‚"""
        self._request_count += 1
        
        # åŸºç¡€å»¶è¿Ÿï¼ˆç§’ï¼‰
        delays = {
            'detail': (10, 15),      # ç¬”è®°è¯¦æƒ…é—´éš”
            'search': (3, 6),       # æœç´¢é¡µæ»šåŠ¨é—´éš”
            'keyword': (20, 40),    # å…³é”®è¯åˆ‡æ¢é—´éš”
        }
        low, high = delays.get(action, (5, 10))
        
        # æ—¶æ®µç³»æ•°ï¼šæ™šé«˜å³°åŠ å€ï¼Œå‡Œæ™¨å‡å°‘
        hour = time.localtime().tm_hour
        if 19 <= hour <= 23:
            low, high = int(low * 1.8), int(high * 1.8)
        elif 0 <= hour <= 7:
            low, high = int(low * 0.7), int(high * 0.7)
        
        # æ¯ 20 æ¬¡è¯·æ±‚å¼ºåˆ¶é•¿ä¼‘æ¯
        if self._request_count % 20 == 0:
            rest = random.uniform(45, 90)
            logger.info(f"   â¸ï¸ å·²è®¿é—® {self._request_count} æ¬¡ï¼Œå¼ºåˆ¶ä¼‘æ¯ {rest:.0f}ç§’...")
            time.sleep(rest)
            return
        
        # è¿ç»­å¤±è´¥æŒ‡æ•°é€€é¿
        if self._consecutive_failures > 0:
            backoff = min(self._consecutive_failures * 10, 120)
            low += backoff
            high += backoff
        
        delay = random.uniform(low, high)
        time.sleep(delay)

    def _check_blocked(self) -> bool:
        """æ£€æµ‹æ˜¯å¦è¢«åçˆ¬æ‹¦æˆª"""
        try:
            url = self.page.url or ''
            text = self.page.run_js('return document.body.innerText.substring(0, 500);') or ''
            
            if any(kw in text for kw in ['éªŒè¯', 'å®‰å…¨æ£€æŸ¥', 'æ“ä½œé¢‘ç¹', 'è¯·ç¨åå†è¯•']):
                self._blocked_count += 1
                self._consecutive_failures += 1
                logger.warning(f"   ğŸ›‘ æ£€æµ‹åˆ°åçˆ¬é™åˆ¶Ã—{self._blocked_count}ï¼æš‚åœ 2-4 åˆ†é’Ÿ...")
                time.sleep(random.uniform(120, 240))
                # å°è¯•è¿”å›é¦–é¡µé‡å»ºä¼šè¯
                self.page.get('https://www.xiaohongshu.com')
                time.sleep(random.uniform(5, 10))
                return True
            
            if '404' in url or 'error' in url:
                return True
        except:
            pass
        return False

    def _safe_int(self, value) -> int:
        try:
            if isinstance(value, int): return value
            if isinstance(value, str):
                v = value.strip()
                if 'ä¸‡' in v: return int(float(v.replace('ä¸‡', '')) * 10000)
                if 'w' in v.lower(): return int(float(v.lower().replace('w', '')) * 10000)
                return int(v)
            return 0
        except: return 0

    def search_notes(self, keyword: str, max_count: int = 20) -> List[Dict]:
        """æœç´¢åˆ—è¡¨ - æå–å¸¦ xsec_token çš„é“¾æ¥"""
        logger.info(f"ğŸ” æœç´¢: {keyword}")
        
        from urllib.parse import quote
        self.page.get(f'https://www.xiaohongshu.com/search_result?keyword={quote(keyword)}&source=web_search_result_notes')
        time.sleep(3)
        
        collected = []
        seen_ids = set()  # æœ¬æ¬¡æœç´¢çš„å»é‡
        page_num = 1
        
        while len(collected) < max_count and page_num <= 8:
            # æå–ç¬”è®°å¡ç‰‡ä¿¡æ¯ - å…³é”®ï¼šè·å–å¸¦ xsec_token çš„ search_result é“¾æ¥
            # åŠ¨æ€æ³¨å…¥é€‰æ‹©å™¨
            search_item_sel = SELECTORS.get('search_note_item', 'section.note-item')
            title_sels = SELECTORS.get('search_note_title', '.title, .note-title, [class*="title"]')
            author_sels = SELECTORS.get('search_note_author', '.author, .nickname, [class*="name"]')
            
            js_extract = f"""
            return (function() {{
                const items = document.querySelectorAll('{search_item_sel}');
                const results = [];
                
                items.forEach((item, index) => {{
                    // ä¼˜å…ˆè·å–å¸¦ xsec_token çš„ search_result é“¾æ¥ï¼ˆåçˆ¬å¿…è¦ï¼‰
                    const searchLink = item.querySelector('a[href*="/search_result/"]');
                    const exploreLink = item.querySelector('a[href*="/explore/"]');
                    
                    if (!searchLink && !exploreLink) return;
                    
                    // ä¼˜å…ˆä½¿ç”¨ search_result é“¾æ¥ï¼ˆå¸¦ xsec_tokenï¼‰
                    const primaryLink = searchLink || exploreLink;
                    const href = primaryLink.getAttribute('href');
                    const exploreHref = exploreLink ? exploreLink.getAttribute('href') : null;
                    
                    // æå–æ ‡é¢˜
                    let title = '';
                    const titleSels = '{title_sels}'.split(', ');
                    for (const sel of titleSels) {{
                        const titleEl = item.querySelector(sel);
                        if (titleEl) {{
                            title = titleEl.innerText;
                            break;
                        }}
                    }}
                    if (!title) title = (item.innerText || '').split('\\n')[0];
                    
                    // æå–ä½œè€…
                    let author = '';
                    const authorSels = '{author_sels}'.split(', ');
                    for (const sel of authorSels) {{
                        const authorEl = item.querySelector(sel);
                        if (authorEl) {{
                            author = authorEl.innerText;
                            break;
                        }}
                    }}
                    
                    results.push({{
                        index: index,
                        href: href,
                        exploreHref: exploreHref,
                        title: title.substring(0, 100),
                        author: author
                    }});
                }});
                return JSON.stringify(results);
            }})();
            """
            
            try:
                res = self.page.run_js(js_extract)
                if res:
                    items = json.loads(res)
                    new_this_round = 0
                    for item in items:
                        href = item.get('href', '')
                        # ä» href æå– note_idï¼ˆå…¼å®¹ explore å’Œ search_result æ ¼å¼ï¼‰
                        note_id = href.split('/')[-1].split('?')[0]
                        if note_id and note_id not in seen_ids:
                            seen_ids.add(note_id)
                            if not self.deduplicator.is_duplicate(note_id):
                                # æ„å»ºå®Œæ•´ URLï¼ˆä¼˜å…ˆå¸¦ xsec_token çš„ search_resultï¼‰
                                full_url = f"https://www.xiaohongshu.com{href}" if href.startswith('/') else href
                                collected.append({
                                    'note_id': note_id,
                                    'title': item['title'],
                                    'author_name': item['author'],
                                    'url': full_url,
                                    'explore_url': f"https://www.xiaohongshu.com{item['exploreHref']}" if item.get('exploreHref') else None,
                                })
                                new_this_round += 1
                    if new_this_round > 0:
                        logger.info(f"   ğŸ“Š ç¬¬{page_num}è½®æ”¶é›†: +{new_this_round} æ¡ (æ€»è®¡: {len(collected)})")
            except Exception as e:
                logger.debug(f"æå–å¼‚å¸¸: {e}")
            
            if len(collected) >= max_count: break
            
            # æ¨¡æ‹Ÿäººç±»ï¼šéšæœºé¼ æ ‡ç§»åŠ¨ + æ»šåŠ¨
            self._random_mouse_move()
            self._human_like_scroll('down', random.randint(400, 700))
            time.sleep(random.uniform(1.5, 3))
            page_num += 1
            
        return collected[:max_count]

    def get_note_detail_pure(self, note_info: Dict) -> Optional[Dict]:
        """è·å–ç¬”è®°è¯¦æƒ… - ç›´æ¥è®¿é—®å¸¦ xsec_token çš„URLï¼ˆæœ€å¯é æ–¹å¼ï¼‰"""
        note_id = note_info['note_id']
        logger.info(f"   ğŸ“– è·å–è¯¦æƒ…: {note_info['title'][:25]}...")
        
        # ========== æ ¸å¿ƒç­–ç•¥ï¼šç›´æ¥è®¿é—®å¸¦ xsec_token çš„URL ==========
        # å°çº¢ä¹¦ç°åœ¨è¦æ±‚æ‰€æœ‰è®¿é—®éƒ½æºå¸¦ xsec_tokenï¼Œå¦åˆ™è¿”å›404
        detail_url = note_info['url']
        
        # æ¨¡æ‹Ÿäººç±»ï¼šå…ˆéšæœºç§»åŠ¨é¼ æ ‡
        self._random_mouse_move()
        time.sleep(random.uniform(0.3, 0.8))
        
        # ç›´æ¥å¯¼èˆªåˆ°è¯¦æƒ…é¡µ (æ–°æ ‡ç­¾é¡µæ¨¡å¼)
        tab = self.page.new_tab(detail_url)
        time.sleep(random.uniform(2, 3.5))
        
        # æ£€æŸ¥æ˜¯å¦è¢«æ‹¦æˆª
        current_url = tab.url or ''
        if '404' in current_url or 'error' in current_url:
            logger.warning(f"   âš ï¸ è¯¦æƒ…é¡µè¢«æ‹¦æˆª(404)ï¼Œå°è¯•exploreé“¾æ¥")
            # å¦‚æœæœ‰å¤‡ç”¨çš„ explore URLï¼Œå°è¯•ä½¿ç”¨
            explore_url = note_info.get('explore_url')
            if explore_url:
                tab.get(explore_url)
                time.sleep(3)
                current_url = tab.url or ''
                if '404' in current_url:
                    logger.warning(f"   âŒ exploreé“¾æ¥ä¹Ÿè¢«æ‹¦æˆªï¼Œè·³è¿‡æ­¤ç¬”è®°")
                    tab.close()
                    return note_info  # è¿”å›åŸºç¡€ä¿¡æ¯
            else:
                logger.warning(f"   âŒ æ— å¤‡ç”¨é“¾æ¥ï¼Œè·³è¿‡æ­¤ç¬”è®°")
                tab.close()
                return note_info

        # ç­‰å¾…è¯¦æƒ…é¡µåŠ è½½
        time.sleep(2)
        
        detail_data = {}
        comments = []

        # ====== ç¬¬ä¸€æ­¥ï¼šSSR æå–ï¼ˆå¿…é¡»åœ¨ä»»ä½•DOMæ“ä½œä¹‹å‰ï¼ï¼‰======
        # å…³é—­å¼¹çª—çš„JSä¼šè¯¯è§¦å‘ç¬”è®°å…³é—­æŒ‰é’®ï¼Œå¯¼è‡´Vueç»„ä»¶å¸è½½ã€SSRæ•°æ®æ¸…ç©º
        # æ‰€ä»¥å¿…é¡»å…ˆæå–SSRæ•°æ®ï¼Œå†åšå…¶ä»–DOMæ“ä½œ
        try:
            ssr_js = f"""
            return (function() {{
                try {{
                    const state = window.__INITIAL_STATE__;
                    if (!state || !state.note || !state.note.noteDetailMap) return null;
                    
                    // 1. ç²¾ç¡®æŸ¥æ‰¾
                    let entry = state.note.noteDetailMap['{note_id}'];
                    
                    // 2. æ¨¡ç³ŠæŸ¥æ‰¾ï¼ˆé¡µé¢å¯èƒ½ç”¨ä¸åŒçš„keyï¼‰
                    if (!entry || !(entry.note || entry.desc)) {{
                        const keys = Object.keys(state.note.noteDetailMap);
                        for (const k of keys) {{
                            const e = state.note.noteDetailMap[k];
                            if (e && (e.note?.desc || e.desc)) {{
                                entry = e;
                                break;
                            }}
                        }}
                    }}
                    
                    if (!entry) return null;
                    const note = entry.note || entry;
                    
                    // æ‰‹åŠ¨æå–å­—æ®µï¼ˆé¿å… Vue Proxy åºåˆ—åŒ–é—®é¢˜ï¼‰
                    const result = {{
                        title: note.title || '',
                        desc: note.desc || '',
                        type: note.type || 'normal',
                        noteId: note.noteId || '',
                        time: note.time || 0,
                        lastUpdateTime: note.lastUpdateTime || 0,
                        tagList: [],
                        interactInfo: {{}}
                    }};
                    
                    // æå–æ ‡ç­¾
                    if (note.tagList && note.tagList.length) {{
                        note.tagList.forEach(t => {{
                            result.tagList.push({{name: t.name || '', id: t.id || ''}});
                        }});
                    }}
                    
                    // æå–äº’åŠ¨æ•°æ®
                    const interact = note.interactInfo || {{}};
                    result.interactInfo = {{
                        likedCount: interact.likedCount || '0',
                        collectedCount: interact.collectedCount || '0',
                        commentCount: interact.commentCount || '0',
                        shareCount: interact.shareCount || '0'
                    }};
                    
                    // æå–ç”¨æˆ·ä¿¡æ¯
                    if (note.user) {{
                        result.user = {{
                            nickname: note.user.nickname || '',
                            userId: note.user.userId || note.user.id || ''
                        }};
                    }}
                    
                    // æå–å›¾ç‰‡åˆ—è¡¨
                    if (note.imageList && note.imageList.length) {{
                        result.imageCount = note.imageList.length;
                    }}
                    
                    return JSON.stringify(result);
                }} catch(e) {{ return JSON.stringify({{error: e.message}}); }}
            }})();
            """
            res = tab.run_js(ssr_js)
            if res:
                data = json.loads(res)
                detail_data = data
                logger.info(f"   âœ… SSR è¢«åŠ¨æå–: desc={len(data.get('desc', ''))}å­—")
                
                # è¿‡æ»¤è§†é¢‘ç¬”è®°
                if detail_data.get('type') == 'video':
                    tab.close()
                    return {'skipped': True, 'reason': 'video'}
        except: pass
        
        # ====== ç¬¬äºŒæ­¥ï¼šDOMæ“ä½œï¼ˆå…³é—­å¼¹çª—ã€å±•å¼€å…¨æ–‡ã€æ»šåŠ¨åŠ è½½è¯„è®ºï¼‰======
        # è¿™äº›æ“ä½œå¯èƒ½å¯¼è‡´Vueç»„ä»¶çŠ¶æ€å˜åŒ–ï¼Œå¿…é¡»åœ¨SSRæå–ä¹‹å
        
        # å…³é—­å¯èƒ½çš„é®ç½©/ç™»å½•å¼¹çª—ï¼ˆæ’é™¤ç¬”è®°è¯¦æƒ…çš„å…³é—­æŒ‰é’®ï¼‰
        try:
            tab.run_js("""
                document.querySelectorAll('.login-close, [class*="login"] [class*="close"]').forEach(e => {
                    if (e.offsetWidth > 0) e.click();
                });
            """)
        except: pass
        
        # å±•å¼€å…¨æ–‡
        try:
            tab.run_js("""
                const expand = document.querySelector('#detail-desc span.expand');
                if (expand) expand.click();
            """)
            time.sleep(0.5)
        except: pass
        
        # æ»šåŠ¨åŠ è½½è¯„è®ºï¼ˆå¤šæ¬¡æ¸è¿›æ»šåŠ¨ï¼Œå°è¯•å¤šç§æ»šåŠ¨å®¹å™¨ï¼‰
        try:
            for scroll_pos in [600, 1200, 1800, 2500, 3500]:
                tab.run_js(f"""
                    // å°è¯•å¤šç§å¯èƒ½çš„æ»šåŠ¨å®¹å™¨
                    const scrollers = [
                        document.querySelector('.note-scroller'),
                        document.querySelector('.note-container'),
                        document.querySelector('#noteContainer'),
                        document.querySelector('[class*="detail"] [class*="scroll"]'),
                        document.querySelector('[class*="comment"]')?.closest('[style*="overflow"]'),
                        document.documentElement
                    ];
                    for (const scroller of scrollers) {{
                        if (scroller) {{
                            scroller.scrollTop = {scroll_pos};
                            break;
                        }}
                    }}
                    // åŒæ—¶å°è¯• window æ»šåŠ¨
                    window.scrollTo(0, {scroll_pos});
                """)
                time.sleep(random.uniform(1.0, 2.0))
        except: pass

        # ====== ç¬¬ä¸‰æ­¥ï¼šDOM æå–ï¼ˆSSRå¤±è´¥æ—¶çš„ä¿åº•ï¼‰======
        if not detail_data.get('desc'):
            try:
                dom_extract = """
                return (function() {
                    const res = {};
                    // æ­£æ–‡
                    const descEl = document.querySelector('#detail-desc') || document.querySelector('.note-text');
                    res.desc = descEl ? descEl.innerText : '';
                    // æ ‡é¢˜
                    const titleEl = document.querySelector('.note-detail-mask .title') || document.querySelector('#detail-title');
                    res.title = titleEl ? titleEl.innerText : '';
                    // æ ‡ç­¾
                    res.tags = Array.from(document.querySelectorAll('.tag-item')).map(e => e.innerText.replace('#',''));
                    // æ—¶é—´
                    const dateEl = document.querySelector('.date');
                    res.time = dateEl ? dateEl.innerText : '';
                    return JSON.stringify(res);
                })();
                """
                dom_res = tab.run_js(dom_extract)
                if dom_res:
                    dom_data = json.loads(dom_res)
                    if dom_data.get('desc'):
                        detail_data.update(dom_data)
                        logger.info(f"   âœ… DOM æå–: desc={len(dom_data['desc'])}å­—")
            except: pass
            
        # ====== ç¬¬å››æ­¥ï¼šè¯„è®ºæå–ï¼ˆSSRä¼˜å…ˆ + DOMå…œåº•ï¼‰======
        
        # 4a. SSR è¯„è®ºæå–ï¼ˆæœ€å¯é  - ä» __INITIAL_STATE__ è·å–ï¼‰
        try:
            ssr_comments_js = f"""
            return (function() {{
                try {{
                    const state = window.__INITIAL_STATE__;
                    if (!state) return null;
                    
                    const coms = [];
                    
                    // æ–¹å¼1ï¼šä» comment.commentsMap æå–
                    if (state.comment) {{
                        let commentData = null;
                        
                        // å°è¯•ä¸åŒçš„æ•°æ®è·¯å¾„
                        if (state.comment.commentsMap) {{
                            commentData = state.comment.commentsMap['{note_id}'];
                            if (!commentData) {{
                                const keys = Object.keys(state.comment.commentsMap);
                                if (keys.length > 0) commentData = state.comment.commentsMap[keys[0]];
                            }}
                        }}
                        
                        // å°è¯• comments æ•°ç»„
                        if (!commentData && state.comment.comments) {{
                            commentData = state.comment.comments;
                        }}
                        
                        if (commentData) {{
                            const commentList = Array.isArray(commentData) ? commentData : (commentData.comments || []);
                            commentList.forEach((c, idx) => {{
                                const content = c.content || c.text || '';
                                const author = c.userInfo?.nickname || c.user?.nickname || c.nickname || 'åŒ¿å';
                                const likeCount = parseInt(c.likeCount || c.like_count || c.likes || 0);
                                const commentId = c.id || c.commentId || c.comment_id || '';
                                const contentLen = content.length;
                                
                                // åªä¿ç•™æœ‰ä»·å€¼çš„è¯„è®ºï¼ˆâ‰¥10å­— æˆ– ç‚¹èµâ‰¥3ï¼‰
                                if (content && (contentLen >= 10 || likeCount >= 3)) {{
                                    coms.push({{content, author_name: author, like_count: likeCount, comment_id: commentId, is_sub: false}});
                                }}
                                
                                // æå–å­è¯„è®º/å›å¤ï¼ˆæ›´æœ‰ä»·å€¼ï¼Œå¾€å¾€æ˜¯è¡¥å……ä¿¡æ¯ï¼‰
                                const subComments = c.subComments || c.subCommentList || c.replies || c.sub_comment_list || [];
                                subComments.forEach(sc => {{
                                    const subContent = sc.content || sc.text || '';
                                    const subAuthor = sc.userInfo?.nickname || sc.user?.nickname || sc.nickname || 'åŒ¿å';
                                    const subLike = parseInt(sc.likeCount || sc.like_count || 0);
                                    const subId = sc.id || sc.commentId || '';
                                    const subLen = subContent.length;
                                    // äºŒçº§è¯„è®ºæ›´å®½æ¾ï¼šâ‰¥8å­—æˆ–ç‚¹èµâ‰¥2
                                    if (subContent && (subLen >= 8 || subLike >= 2)) {{
                                        coms.push({{content: subContent, author_name: subAuthor, like_count: subLike, comment_id: subId, is_sub: true}});
                                    }}
                                }});
                            }});
                        }}
                    }}
                    
                    // æ–¹å¼2ï¼šä» noteDetailMap ä¸­æå–è¯„è®ºç›¸å…³æ•°æ®
                    if (coms.length === 0 && state.note && state.note.noteDetailMap) {{
                        const keys = Object.keys(state.note.noteDetailMap);
                        for (const k of keys) {{
                            const entry = state.note.noteDetailMap[k];
                            const note = entry?.note || entry;
                            if (note && note.comments) {{
                                note.comments.forEach(c => {{
                                    const content = c.content || c.text || '';
                                    const author = c.userInfo?.nickname || c.nickname || 'åŒ¿å';
                                    if (content) {{
                                        coms.push({{content, author_name: author, like_count: parseInt(c.likeCount || 0), comment_id: c.id || ''}});
                                    }}
                                }});
                            }}
                        }}
                    }}
                    
                    return coms.length > 0 ? JSON.stringify(coms) : null;
                }} catch(e) {{ return null; }}
            }})();
            """
            ssr_c_res = tab.run_js(ssr_comments_js)
            if ssr_c_res:
                ssr_comments = json.loads(ssr_c_res)
                if ssr_comments:
                    # ç»Ÿè®¡ä¸€çº§å’ŒäºŒçº§è¯„è®º
                    primary = sum(1 for c in ssr_comments if not c.get('is_sub'))
                    sub = sum(1 for c in ssr_comments if c.get('is_sub'))
                    comments = ssr_comments
                    logger.info(f"   ğŸ’¬ SSRè¯„è®ºæå–: {len(comments)}æ¡ (ä¸€çº§{primary}+äºŒçº§{sub})")
        except Exception as e:
            logger.debug(f"SSRè¯„è®ºæå–å¼‚å¸¸: {e}")
        
        # 4b. DOM è¯„è®ºæå–ï¼ˆSSRå¤±è´¥æ—¶çš„å…œåº•ï¼Œä½¿ç”¨å¤šç§é€‰æ‹©å™¨ï¼‰
        if not comments:
            try:
                comments_js = """
                return (function() {
                    const coms = [];
                    const seen = new Set();
                    
                    // å¤šç§è¯„è®ºé€‰æ‹©å™¨ï¼ˆå…¼å®¹ä¸åŒç‰ˆæœ¬çš„å°çº¢ä¹¦å‰ç«¯ï¼‰
                    const selectors = [
                        '.comment-item',
                        '.comment-inner-container',
                        '[class*="commentItem"]',
                        '[class*="comment-item"]',
                        '.parent-comment',
                        '[class*="CommentItem"]'
                    ];
                    
                    let commentEls = [];
                    for (const sel of selectors) {
                        const els = document.querySelectorAll(sel);
                        if (els.length > 0) {
                            commentEls = els;
                            break;
                        }
                    }
                    
                    commentEls.forEach((el, idx) => {
                        // å¤šç§å†…å®¹é€‰æ‹©å™¨
                        let content = '';
                        const contentSels = ['.content', '.note-text', '[class*="content"]', '[class*="text"]', 'p'];
                        for (const sel of contentSels) {
                            const contentEl = el.querySelector(sel);
                            if (contentEl && contentEl.innerText.trim()) {
                                content = contentEl.innerText.trim();
                                break;
                            }
                        }
                        
                        // å¤šç§ä½œè€…é€‰æ‹©å™¨
                        let author = 'åŒ¿å';
                        const authorSels = ['.name', '.author', '.nickname', '.user-name', '[class*="name"]', '[class*="author"]'];
                        for (const sel of authorSels) {
                            const authorEl = el.querySelector(sel);
                            if (authorEl && authorEl.innerText.trim()) {
                                author = authorEl.innerText.trim();
                                break;
                            }
                        }
                        
                        // å¤šç§ç‚¹èµæ•°é€‰æ‹©å™¨
                        let likeNum = 0;
                        const likeSels = ['.like-count', '.like span', '[class*="like"] span', '[class*="count"]'];
                        for (const sel of likeSels) {
                            const likeEl = el.querySelector(sel);
                            if (likeEl) {
                                const likeText = likeEl.innerText.trim();
                                if (likeText.includes('ä¸‡')) {
                                    likeNum = Math.round(parseFloat(likeText) * 10000);
                                } else {
                                    likeNum = parseInt(likeText) || 0;
                                }
                                break;
                            }
                        }
                        
                        // å»é‡ + è´¨é‡è¿‡æ»¤ï¼ˆåŸºäºå†…å®¹ï¼‰
                        if (content && !seen.has(content)) {
                            // åªä¿ç•™æœ‰ä»·å€¼çš„è¯„è®ºï¼šâ‰¥10å­— æˆ– ç‚¹èµâ‰¥3
                            if (content.length >= 10 || likeNum >= 3) {
                                seen.add(content);
                                coms.push({content, author_name: author, like_count: likeNum});
                            }
                        }
                    });
                    
                    // æŒ‰ç‚¹èµæ•°é™åº
                    coms.sort((a, b) => b.like_count - a.like_count);
                    return JSON.stringify(coms);
                })();
                """
                c_res = tab.run_js(comments_js)
                if c_res:
                    dom_comments = json.loads(c_res)
                    if dom_comments:
                        comments = dom_comments
                        avg_len = sum(len(c.get('content', '')) for c in comments) / len(comments) if comments else 0
                        logger.info(f"   ğŸ’¬ DOMè¯„è®ºæå–: {len(comments)}æ¡ (å¹³å‡{avg_len:.0f}å­—)")
            except Exception as e:
                logger.debug(f"DOMè¯„è®ºæå–å¼‚å¸¸: {e}")

        # é€€å‡ºè¯¦æƒ…
        try:
            tab.close()
        except: pass
        time.sleep(0.5)
        
        # ç»„è£…
        full_note = note_info.copy()
        full_note['desc'] = detail_data.get('desc', '')
        full_note['title'] = detail_data.get('title', full_note.get('title', ''))
        
        # æå–æ ‡ç­¾ï¼ˆå…¼å®¹ tagList å’Œ tags ä¸¤ç§æ ¼å¼ï¼‰
        raw_tags = detail_data.get('tagList', detail_data.get('tags', []))
        if raw_tags and isinstance(raw_tags[0], dict):
            full_note['tags'] = [t.get('name', '') for t in raw_tags if t.get('name')]
        elif raw_tags:
            full_note['tags'] = raw_tags
        else:
            full_note['tags'] = []
             
        full_note['time'] = str(detail_data.get('time', detail_data.get('lastUpdateTime', detail_data.get('last_update_time', ''))))
        
        # äº’åŠ¨æ•°æ®ï¼ˆå…¼å®¹ interactInfo å’Œ interact_infoï¼‰
        interact = detail_data.get('interactInfo', detail_data.get('interact_info', {}))
        full_note['liked_count'] = self._safe_int(interact.get('likedCount', interact.get('liked_count', 0)))
        full_note['collected_count'] = self._safe_int(interact.get('collectedCount', interact.get('collected_count', 0)))
        full_note['comment_count'] = self._safe_int(interact.get('commentCount', interact.get('comment_count', 0)))
        
        # è®¡ç®—æ€»äº’åŠ¨é‡å’Œæµé‡ç­‰çº§ï¼ˆç”¨äºRAGç­›é€‰ä¼˜è´¨å†…å®¹ï¼‰
        total_interaction = full_note['liked_count'] + full_note['collected_count'] + full_note['comment_count']
        full_note['total_interaction'] = total_interaction
        
        # æµé‡ç­‰çº§åˆ†ç±»ï¼ˆçˆ†æ¬¾/ä¼˜è´¨/æ™®é€š/ä½è´¨ï¼‰
        if total_interaction >= 10000:
            full_note['traffic_level'] = 'çˆ†æ¬¾'
        elif total_interaction >= 1000:
            full_note['traffic_level'] = 'ä¼˜è´¨'
        elif total_interaction >= 100:
            full_note['traffic_level'] = 'æ™®é€š'
        else:
            full_note['traffic_level'] = 'ä½è´¨'
        
        # æå–ä½œè€…ä¿¡æ¯ï¼ˆauthor_idï¼‰
        user_info = detail_data.get('user', {})
        if user_info:
            full_note['author_id'] = user_info.get('userId', '')
            full_note['author_name'] = user_info.get('nickname', full_note.get('author_name', ''))
        
        # è¯„è®ºåå¤„ç†ï¼šè´¨é‡ç­›é€‰ + ç”Ÿæˆå”¯ä¸€ID
        if comments:
            # æŒ‰ä»·å€¼æ’åºï¼šä¼˜å…ˆé•¿è¯„è®ºå’Œé«˜èµè¯„è®º
            comments_sorted = sorted(comments, key=lambda c: (len(c.get('content', '')), c.get('like_count', 0)), reverse=True)
            # ä¿ç•™å‰50æ¡æœ€æœ‰ä»·å€¼çš„ï¼ˆé¿å…è¿‡å¤šä½è´¨é‡è¯„è®ºï¼‰
            comments = comments_sorted[:50]
            
            # ä¸ºæ¯æ¡è¯„è®ºç”Ÿæˆå”¯ä¸€ID
            for idx, c in enumerate(comments):
                if not c.get('comment_id'):
                    content_hash = hashlib.md5(f"{note_id}_{c.get('content', '')}_{idx}".encode()).hexdigest()[:12]
                    c['comment_id'] = f"{note_id}_{content_hash}"
        
        full_note['comments_data'] = comments
        full_note['full_text'] = f"{full_note['title']} {full_note['desc']} {' '.join(full_note['tags'])}"
        
        return full_note

    def _clean_text(self, text: str) -> str:
        """æ¸…æ´—æ–‡æœ¬ï¼šå»é™¤Emojiã€æ— æ„ä¹‰æ ‡ç­¾ã€å¤šä½™ç©ºæ ¼"""
        if not text:
            return ""
            
        # 1. å»é™¤Emoji (ä¿ç•™å¸¸è§æ ‡ç‚¹)
        # è¿™æ˜¯ä¸€ä¸ªç®€å•çš„èŒƒå›´ï¼Œè¦†ç›–å¤§å¤šæ•°Emoji
        try:
            # è¿‡æ»¤æ‰éBMPå­—ç¬¦ï¼ˆé€šå¸¸æ˜¯Emojiï¼‰
            text = "".join(c for c in text if c <= "\uFFFF")
        except: pass
        
        # 2. è§„èŒƒåŒ–æ ‡ç­¾æ ¼å¼: #æ ‡ç­¾ -> [æ ‡ç­¾]
        text = re.sub(r'#(\S+)', r'[\1]', text)
        
        # 3. å»é™¤å¤šä½™ç©ºç™½
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text

    def _to_storage_note(self, note: Dict) -> Dict:
        """è½¬æ¢ä¸ºæ ‡å‡†å­˜å‚¨æ ¼å¼"""
        clean_title = self._clean_text(note.get('title', ''))
        clean_desc = self._clean_text(note.get('desc', ''))
        clean_tags = [self._clean_text(t) for t in note.get('tags', [])]
        
        # é‡æ–°ç»„åˆ full_text
        full_text = f"{clean_title} {clean_desc} {' '.join(clean_tags)}"
        
        return {
            'note_id': note.get('note_id', ''),
            'url': note.get('url', ''),
            'title': note.get('title', ''),
            'desc': note.get('desc', ''),
            'note_type': note.get('type', 'normal'),
            'author_name': note.get('author_name', ''),
            'author_id': note.get('author_id', ''),
            'liked_count': note.get('liked_count', 0),
            'collected_count': note.get('collected_count', 0),
            'comment_count': note.get('comment_count', 0),
            'total_interaction': note.get('total_interaction', 0),
            'traffic_level': note.get('traffic_level', ''),
            'tags': note.get('tags', []),
            'upload_time': str(note.get('time', '')),
            'full_text': full_text,
            'keyword_source': note.get('keyword_source', '')
        }

    def crawl(self, keywords: List[str], limit: int = 20, daily_limit: int = 0,
              min_likes: int = 0, warmup: bool = True, shuffle: bool = True):
        """
        æ‰¹é‡çˆ¬å–
        
        Args:
            keywords: å…³é”®è¯åˆ—è¡¨
            limit: æ¯ä¸ªå…³é”®è¯æœ€å¤šçˆ¬å–ç¬”è®°æ•°
            daily_limit: æ¯æ—¥æœ€å¤šçˆ®å–æ€»æ•°ï¼ˆ0=æ— é™åˆ¶ï¼‰
            min_likes: æœ€å°‘ç‚¹èµæ•°è¿‡æ»¤ï¼ˆè·³è¿‡ä½äº’åŠ¨ç¬”è®°å‡å°‘è¯·æ±‚ï¼‰
            warmup: æ˜¯å¦é¢„çƒ­ä¼šè¯
            shuffle: æ˜¯å¦éšæœºæ‰“ä¹±å…³é”®è¯é¡ºåº
        """
        self.stats["start_time"] = datetime.now()
        self.init_browser()
        
        # åŠ è½½è¿›åº¦ï¼ˆæ”¯æŒæ–­ç‚¹ç»­çˆ¬ï¼‰
        done_keywords, loaded_daily_count = self._load_progress()
        if done_keywords:
            original_count = len(keywords)
            keywords = [kw for kw in keywords if kw not in done_keywords]
            if original_count > len(keywords):
                logger.info(f"ğŸ“‚ æ–­ç‚¹ç»­çˆ¬: è·³è¿‡å·²å®Œæˆçš„ {original_count - len(keywords)} ä¸ªå…³é”®è¯")
        
        if not keywords:
            logger.info("âœ… æ‰€æœ‰å…³é”®è¯å·²å®Œæˆï¼Œæ— éœ€é‡å¤çˆ¬å–")
            return
        
        # éšæœºæ‰“ä¹±å…³é”®è¯é¡ºåºï¼ˆé¿å…ç³»ç»Ÿæ€§æ‰«æç‰¹å¾ï¼‰
        if shuffle:
            random.shuffle(keywords)
            logger.info(f"ğŸ² å·²éšæœºæ‰“ä¹± {len(keywords)} ä¸ªå…³é”®è¯é¡ºåº")
        
        # ä¼šè¯é¢„çƒ­
        if warmup:
            self._warmup_session()
        
        daily_count = loaded_daily_count  # æ¢å¤å½“æ—¥å·²çˆ¬å–æ•°
        if daily_count > 0:
            logger.info(f"ğŸ“Š ä»Šæ—¥å·²çˆ¬å– {daily_count} æ¡ï¼Œç»§ç»­ç´¯è®¡...")
        
        for i, kw in enumerate(keywords):
            # æ¯æ—¥ä¸Šé™æ£€æŸ¥
            if daily_limit > 0 and daily_count >= daily_limit:
                logger.info(f"\nğŸ“Š å·²è¾¾åˆ°æ¯æ—¥ä¸Šé™ {daily_limit} æ¡ï¼Œä»Šæ—¥çˆ¬å–ç»“æŸ")
                logger.info(f"   å‰©ä½™ {len(keywords) - i} ä¸ªå…³é”®è¯å°†åœ¨ä¸‹æ¬¡è¿è¡Œæ—¶ç»§ç»­")
                break
            
            logger.info(f"\nğŸ“ è¿›åº¦: {i+1}/{len(keywords)} | å…³é”®è¯: {kw} | ä»Šæ—¥å·²çˆ¬: {daily_count}")
            
            # å…³é”®è¯é—´å†·å´ï¼ˆç¬¬ä¸€ä¸ªå…³é”®è¯ä¸ç­‰ï¼‰
            if i > 0:
                self._smart_delay('keyword')
            
            # ç†”æ–­æ£€æŸ¥ï¼šè¿ç»­å¤±è´¥å¤ªå¤šåˆ™æš‚åœ
            if self._consecutive_failures >= 5:
                pause = random.uniform(180, 300)
                logger.warning(f"   ğŸ›‘ è¿ç»­å¤±è´¥ {self._consecutive_failures} æ¬¡ï¼Œç†”æ–­ä¼‘æ¯ {pause:.0f}ç§’...")
                time.sleep(pause)
                self._consecutive_failures = 0
                # é‡å»ºä¼šè¯
                self.page.get('https://www.xiaohongshu.com')
                time.sleep(5)
            
            notes = self.search_notes(kw, limit)
            
            if not notes:
                self._consecutive_failures += 1
                self.stats["failed_keywords"] += 1
                logger.warning(f"   âš ï¸ æ— æœç´¢ç»“æœï¼Œè·³è¿‡")
                continue
            
            self._consecutive_failures = 0  # æœç´¢æˆåŠŸåˆ™é‡ç½®
            
            kw_note_count = 0
            for j, note in enumerate(notes):
                # æ¯æ—¥ä¸Šé™æ£€æŸ¥
                if daily_limit > 0 and daily_count >= daily_limit:
                    break
                
                logger.info(f"   ğŸ“– [{j+1}/{len(notes)}] {note['title'][:30]}...")
                
                # åçˆ¬æ£€æµ‹
                if self._check_blocked():
                    logger.warning(f"   âš ï¸ è§¦å‘åçˆ¬ï¼Œæœ¬å…³é”®è¯å‰©ä½™ç¬”è®°è·³è¿‡")
                    break
                
                # è®¿é—®è¯¦æƒ…é¡µè·å–å®Œæ•´æ•°æ®
                full_note = self.get_note_detail_pure(note)
                
                # è·³è¿‡è¢«æ ‡è®°çš„ç¬”è®°ï¼ˆå¦‚è§†é¢‘ï¼‰
                if full_note and full_note.get('skipped'):
                    logger.info(f"      â­ï¸ è·³è¿‡è§†é¢‘ç¬”è®°")
                    continue
                
                if full_note and self.storage:
                    # æœ€å°‘ç‚¹èµè¿‡æ»¤ï¼ˆå‡å°‘æ— æ•ˆè¯·æ±‚ï¼‰
                    liked = full_note.get('liked_count', 0)
                    if min_likes > 0 and liked < min_likes:
                        logger.debug(f"      â­ï¸ ç‚¹èµ{liked}<{min_likes}ï¼Œè·³è¿‡ä½äº’åŠ¨ç¬”è®°")
                        continue
                    
                    # è®°å½•å…³é”®è¯æ¥æº
                    full_note['keyword_source'] = kw
                    
                    storage_note = self._to_storage_note(full_note)
                    self.storage.add_note(storage_note)
                    if full_note.get('comments_data'):
                        self.storage.add_comments(full_note['note_id'], full_note['comments_data'])
                    self.stats["total_notes"] += 1
                    daily_count += 1
                    kw_note_count += 1
                    desc_len = len(full_note.get('desc', ''))
                    comment_cnt = len(full_note.get('comments_data', []))
                    if desc_len > 0:
                        self._consecutive_failures = 0
                        logger.info(f"      âœ… æ­£æ–‡: {desc_len}å­— | â¤ï¸{liked} | ğŸ’¬{comment_cnt}æ¡")
                    else:
                        self._consecutive_failures += 1
                        comment_cnt = len(full_note.get('comments_data', []))
                        logger.warning(f"      âš ï¸ æœªè·å–åˆ°æ­£æ–‡ | ğŸ’¬{comment_cnt}æ¡è¯„è®º")
                else:
                    self._consecutive_failures += 1
                
                # æ™ºèƒ½å»¶è¿Ÿï¼ˆæ ¹æ®æ—¶æ®µ/è¯·æ±‚æ¬¡æ•°/å¤±è´¥ç‡åŠ¨æ€è°ƒèŠ‚ï¼‰
                self._smart_delay('detail')
            
            # æ ‡è®°å…³é”®è¯å®Œæˆå¹¶ä¿å­˜è¿›åº¦
            done_keywords.add(kw)
            self._save_progress(done_keywords, daily_count)
            
            logger.info(f"   âœ… å…³é”®è¯ã€Œ{kw}ã€å®Œæˆ: æ”¶å½• {kw_note_count} æ¡")
            
            # æ¯ 3 ä¸ªå…³é”®è¯åé¢å¤–ä¼‘æ¯
            if (i + 1) % 3 == 0 and i + 1 < len(keywords):
                rest = random.uniform(30, 60)
                logger.info(f"   â˜• æ¯3ä¸ªå…³é”®è¯ä¼‘æ¯ {rest:.0f}ç§’...")
                time.sleep(rest)

        self._print_stats(daily_count, daily_limit)
        
        # å®Œæˆå­˜å‚¨ï¼ˆJSON/Excel éœ€è¦æœ€ç»ˆå†™å…¥ï¼‰
        if self.storage:
            self.storage.finalize()
        
        
    def _print_stats(self, daily_count: int = 0, daily_limit: int = 0):
        duration = (datetime.now() - self.stats["start_time"]).total_seconds()
        limit_info = f"  æ¯æ—¥ä¸Šé™: {daily_count}/{daily_limit}\n" if daily_limit > 0 else ""
        logger.info(
            f"\n{'#'*60}\n"
            f"# ğŸ“Š çˆ¬å–ç»“æŸ\n"
            f"  æ”¶å½•ç¬”è®°æ•°: {self.stats['total_notes']}\n"
            f"{limit_info}"
            f"  å¤±è´¥å…³é”®è¯: {self.stats['failed_keywords']}\n"
            f"  è§¦å‘åçˆ¬: {self._blocked_count} æ¬¡\n"
            f"  æ€»è€—æ—¶: {duration/60:.1f} åˆ†é’Ÿ\n"
            f"{'#'*60}"
        )

def main():
    parser = argparse.ArgumentParser(description='DrissionPage å°çº¢ä¹¦çˆ¬è™« (å¤šæ ¼å¼å­˜å‚¨ç‰ˆ)')
    parser.add_argument('--keywords', '-k', nargs='+', help='å…³é”®è¯åˆ—è¡¨')
    parser.add_argument('--limit', '-l', type=int, default=20, help='æ¯ä¸ªå…³é”®è¯æœ€å¤šçˆ¬å–æ•°é‡')
    parser.add_argument('--daily-limit', '-d', type=int, default=0,
                        help='æ¯æ—¥æœ€å¤šçˆ¬å–æ€»æ•°ï¼ˆ0=æ— é™åˆ¶ï¼Œæ¨è 50-100ï¼‰')
    parser.add_argument('--min-likes', type=int, default=0,
                        help='æœ€å°‘ç‚¹èµæ•°è¿‡æ»¤ï¼ˆè·³è¿‡ä½äº’åŠ¨ç¬”è®°ï¼Œå‡å°‘è¯·æ±‚ï¼‰')
    parser.add_argument('--storage', '-s', type=str, default='sqlite',
                        choices=['csv', 'json', 'excel', 'sqlite'],
                        help='å­˜å‚¨æ ¼å¼ (csv/json/excel/sqliteï¼Œé»˜è®¤: sqlite)')
    parser.add_argument('--output', '-o', type=str, default='datas',
                        help='è¾“å‡ºç›®å½•ï¼ˆé»˜è®¤: datasï¼‰')
    parser.add_argument('--no-warmup', action='store_true', help='è·³è¿‡ä¼šè¯é¢„çƒ­')
    parser.add_argument('--no-shuffle', action='store_true', help='ä¸æ‰“ä¹±å…³é”®è¯é¡ºåº')
    args = parser.parse_args()
    
    spider = DrissionXHSSpider(storage_type=args.storage, output_dir=args.output)
    keywords = args.keywords if args.keywords else ["æ¾³æ´²ç•™å­¦"]
    
    logger.info(f"ğŸ“¦ å­˜å‚¨æ ¼å¼: {args.storage.upper()}")
    logger.info(f"ğŸ“‚ è¾“å‡ºç›®å½•: {args.output}")
        
    spider.crawl(
        keywords, limit=args.limit,
        daily_limit=args.daily_limit,
        min_likes=args.min_likes,
        warmup=not args.no_warmup,
        shuffle=not args.no_shuffle
    )

if __name__ == "__main__":
    main()
