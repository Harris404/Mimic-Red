# 小红书爬虫使用说明

## 🍪 账号与Cookie管理

由于本项目直接接管真实的 Chrome 浏览器，因此**账号切换**和**Cookie 清理**的操作与平时使用浏览器完全一致：

### 1. 切换账号
1. 在启动的 Chrome 窗口中，点击页面右上角的头像。
2. 选择“退出登录”。
3. 页面刷新后，使用新的账号扫码登录即可。

### 2. 清除 Cookie (解决登录异常)
如果遇到登录死循环或状态异常，建议清除 Cookie：
1. 在 Chrome 地址栏左侧，点击 🔒 (锁图标)。
2. 点击 **"Cookie 和网站数据"**。
3. 点击 **"管理 Cookie 和网站数据"**。
4. 点击垃圾桶图标删除所有记录，然后刷新页面重新登录。

> **💡 彻底重置**: 如果想完全重置环境，可以关闭 Chrome 后删除用户数据目录：
> - Mac/Linux: `rm -rf ~/.chrome-debug-profile`
> - Windows: 删除 `%USERPROFILE%\.chrome-debug-profile` 文件夹

## 📦 支持的存储格式

新版爬虫支持 4 种数据存储格式，您可以根据需求选择：

| 格式 | 特点 | 适用场景 |
|------|------|----------|
| **SQLite** | 关系型数据库，支持高级查询 | 大量数据，需要复杂查询 |
| **CSV** | 表格格式，Excel可直接打开 | 数据分析、导入其他工具 |
| **JSON** | 结构化文本，易于程序读取 | 数据交换、API对接 |
| **Excel** | 多工作表，格式美观 | 报告、展示、简单分析 |

## 🚀 快速开始

### 1. 启动 Chrome（调试模式）
```bash
./start_chrome.sh
```

### 2. 运行爬虫

#### 使用 SQLite（默认）
```bash
python mimic_red_engine.py --keywords "澳洲留学" "悉尼大学" --limit 20
```

#### 使用 CSV
```bash
python mimic_red_engine.py --keywords "澳洲留学" --storage csv --limit 20
```

#### 使用 JSON
```bash
python mimic_red_engine.py --keywords "澳洲留学" --storage json --limit 20
```

#### 使用 Excel
```bash
python mimic_red_engine.py --keywords "澳洲留学" --storage excel --limit 20
```

## 📋 命令行参数

| 参数 | 简写 | 说明 | 默认值 |
|------|------|------|--------|
| `--keywords` | `-k` | 关键词列表（空格分隔） | "澳洲留学" |
| `--storage` | `-s` | 存储格式 (csv/json/excel/sqlite) | sqlite |
| `--output` | `-o` | 输出基础目录 | datas |
| `--limit` | `-l` | 每个关键词爬取数量 | 20 |
| `--daily-limit` | `-d` | 每日总爬取上限 | 0（无限制） |
| `--min-likes` | | 最少点赞数过滤 | 0 |
| `--no-warmup` | | 跳过会话预热 | False |
| `--no-shuffle` | | 不打乱关键词顺序 | False |

## 💡 使用示例

### 示例 1：爬取多个关键词，保存为 CSV
```bash
python mimic_red_engine.py \
  --keywords "悉尼大学" "墨尔本大学" "昆士兰大学" \
  --storage csv \
  --limit 30 \
  --output results
```

### 示例 2：每日限量爬取，保存为 Excel
```bash
python mimic_red_engine.py \
  --keywords "澳洲留学" \
  --storage excel \
  --daily-limit 50 \
  --min-likes 100
```

### 示例 3：从数据库读取关键词，保存为 JSON
```bash自定义输出基础目录
```bash
python mimic_red_engine.py \
  --keywords "澳洲留学" \
  --storage json \
  --output my_data \
  --limit 20
```
输出路径：`my_data/json_datas/notes_xxx.json
## 📁 输出文件说明

### 文件夹结构
根据选择的存储格式，数据会自动存储到对应的专用文件夹：
```
项目根目录/
├── sqlite_datas/     # SQLite 数据库文件
├── csv_datas/        # CSV 格式文件
├── json_datas/       # JSON 格式文件
└── excel_datas/      # Excel 格式文件
```

### SQLite 输出
```
csv_sqlite_datas/
  └── notes.db          # SQLite 数据库文件
```

数据库包含两张表：
- `notes`: 笔记主表
- `comments`: 评论表

### CSV 输出
```
datas/
  ├── notes_20260210_120000.csv      # 笔记数据
  └── comments_20260210_120000.csv   # 评论数据
```

### JSON 输出
```
json_datas/
  └── notes_20260210_120000.json     # 包含笔记和评论的完整数据
```

JSON 文件结构：
```json
{
  "notes": [...],
  "comments": [...],
  "crawl_time": "2026-02-10T12:00:00",
  "total_notes": 100,
  "total_comments": 500
}
```

### Excel 输出
```
excel_datas/
  └── notes_20260210_120000.xlsx     # Excel 工作簿
```

Excel 包含两个工作表：
- `Notes`: 笔记数据
- `Comments`: 评论数据

## 🔧 数据字段说明

### 笔记（Notes）
- `note_id`: 笔记唯一ID
- `url`: 笔记链接
- `title`: 标题
- `desc`: 正文内容
- `note_type`: 笔记类型（normal/video）
- `author_id`: 作者ID
- `author_name`: 作者昵称
- `liked_count`: 点赞数
- `collected_count`: 收藏数
- `comment_count`: 评论数
- `total_interaction`: 总互动量
- `traffic_level`: 流量等级（爆款/优质/普通/低质）
- `tags`: 标签（CSV用|分隔，其他为数组）
- `upload_time`: 发布时间
- `keyword_source`: 来源关键词
- `full_text`: 完整文本（标题+正文+标签）

### 评论（Comments）
- `comment_id`: 评论唯一ID
- `note_id`: 所属笔记ID
- `content`: 评论内容
- `author_name`: 评论者昵称
- `like_count`: 点赞数

## ⚠️ 注意事项

1. **Excel 格式需要额外安装依赖**：
   ```bash
   pip install pandas openpyxl
   ```

2. **CSV 文件使用 UTF-8-BOM 编码**，Excel 可直接打开不会乱码

3. **JSON 和 Excel 格式**会在爬取结束时一次性写入文件

4. **SQLite 和 CSV 格式**实时写入，支持断点续爬

5. **数据去重**（2026年增强）：
   - **SQLite**: 自动跨运行去重（推荐长期使用）
   - **CSV/JSON**: 启动时扫描所有历史文件加载笔记ID，支持跨运行去重
   - **Excel**: 当前会话内去重

6. **依赖版本管理**（新增）：
   ```bash
   # 生产环境推荐：使用锁定版本
   pip install -r requirements-lock.txt
   
   # 开发环境：使用最小版本约束
   pip install -r requirements.txt
   ```

## 🚀 进阶配置（2026年新增）

### 安全爬取建议

根据2026年最新反爬研究，以下配置可降低封号风险：

#### 1. 按场景选择每日限额

```bash
# 新账号测试期（建立信任基线）
python mimic_red_engine.py -k "澳洲留学" -l 10 -d 50

# 稳定运行期（推荐配置）
python mimic_red_engine.py -k "澳洲留学" -l 20 -d 100 --storage sqlite

# 紧急数据需求（高风险，需监控）
python mimic_red_engine.py -k "澳洲留学" -l 30 -d 200 --min-likes 100
```

#### 2. 避开高峰时段

小红书系统在以下时段风控更严格：
- **晚高峰**: 19:00-23:00（延迟会自动×1.8）
- **午休时段**: 12:00-14:00（建议手动暂停）
- **周末/节假日**: 流量更大，系统更敏感

**建议运行时段**：
- 工作日上午：09:00-11:00
- 工作日下午：15:00-17:00
- 深夜/凌晨：01:00-06:00（延迟自动×0.5）

#### 3. 长期使用策略

```bash
# 每周至少休息1-2天，模拟真实用户
# 周一-周五爬取，周六日休息

# 周一
python mimic_red_engine.py -k "关键词1" -d 100

# 周二
python mimic_red_engine.py -k "关键词2" -d 100

# ...

# 周六日：完全停止爬取
```

### 风险监控

如果遇到以下情况，**立即停止爬取**：

1. **连续出现404错误**（超过3次）
2. **触发验证码**
3. **搜索结果为空**（但手动搜索有结果）
4. **笔记详情页无法加载**

**恢复措施**：
1. 停止爬取2-4小时
2. 清除Cookie重新登录
3. 更换IP地址（如使用代理）
4. 降低每日限额（减少50%）

### 代理配置（推荐）

虽然当前版本未内置代理支持，但您可以通过以下方式配置：

```bash
# 方案1：系统级代理（Mac）
export http_proxy=http://127.0.0.1:7890
export https_proxy=http://127.0.0.1:7890

# 方案2：Chrome启动参数（修改 start_chrome.sh）
/Applications/Google\ Chrome.app/Contents/MacOS/Google\ Chrome \
    --remote-debugging-port=9222 \
    --proxy-server="http://your-proxy-ip:port" \
    --user-data-dir="$HOME/.chrome-debug-profile"
```

**代理选择建议**：
- ❌ 数据中心IP（AWS/阿里云）：立即被标记
- ✅ 住宅IP（家庭宽带）：成功率85-92%
- 💰 成本：约¥300-500/月（5GB流量）

### 法律合规提醒

⚠️ **重要**: 爬取数据需遵守中国法律法规

| 场景 | 合法性 | 风险等级 |
|------|--------|---------|
| 个人学习/学术研究 | ✅ 合法 | 🟢 低 |
| 商业数据分析（不盈利） | ⚠️ 灰色地带 | 🟡 中 |
| 数据转售/API服务 | ❌ 违法 | 🔴 极高 |
| 累计爬取 > 100万条 | ❌ 刑事案件 | 🚫 刑事追诉 |

**建议**：
- 仅用于个人学习
- 控制总量 < 10万条
- 不得爬取用户隐私（联系方式、精准位置）
- 不得用于商业盈利

## 🔄 数据转换

如果需要在不同格式间转换，可以使用以下工具：

### SQLite → CSV
```bashsqlite_
sqlite3 -header -csv datas/notes.db "SELECT * FROM notes;" > notes.csv
```

### CSV → Excel（使用 Python）
```python
import pandas as pd
df = pd.read_csv('notes.csv')
df.to_excel('notes.xlsx', index=False)
```

### JSON → CSV（使用 Python）
```python
import pandas as pd
data = pd.read_json('notes.json')
data['notes'].to_csv('notes.csv', index=False)
```


## 🆘 常见问题

**Q: Excel 格式报错找不到 pandas？**
A: 运行 `pip install pandas openpyxl`

**Q: CSV 文件在 Excel 中打开乱码？**
A: 使用 UTF-8-BOM 编码保存，应该不会乱码。如果还有问题，用 Excel 的"数据"-"从文本导入"功能。

**Q: 如何查看 SQLite 数据？**sqlite_datas/notes.db`

**Q: 能同时使用多种格式吗？**
A: 当前版本每次运行只能选择一种格式，如需多种格式，请多次运行。

**Q: 不同格式的数据存在哪里？**
A: 根据格式自动存储到专用文件夹：`sqlite_datas/`、`csv_datas/`、`json_datas/`、`excel_datas/`

## 🔧 常见问题排查

### 1. 无法连接浏览器 (Connection refused)
**现象**: `❌ 接管失败: ... Connection refused`
**原因**: Chrome 未启动或未开启调试端口 9222。
**解决**:
- 务必先运行 `start_chrome.sh` (Mac) or `start_chrome.bat` (Win)。
- 检查 Chrome 是否占用了 9222 端口（有时残留进程会导致冲突，重启电脑可解）。

### 2. 缺少依赖 (ModuleNotFoundError)
**现象**: `❌ 导入依赖失败: No module named ...`
**解决**:
- 运行 `pip install -r requirements.txt`。
- Excel 导出功能需要额外安装: `pip install pandas openpyxl`。

### 3. 数据提取失败 (全是空数据)
**现象**: 日志显示 `⚠️ 未获取到正文`，但浏览器中可以看到内容。
**原因**: 小红书网页结构更新，CSS 选择器失效。
**解决**:
- 检查 `selectors.json` 文件。
- 手动在浏览器开发者工具中确认最新的 CSS 类名，并更新配置文件。

### 4. 频繁触发反爬 (404/验证码)
**现象**: 连续出现 `🛑 检测到反爬限制` 或 `404`。
**解决**:
- 立即停止爬取，休息几小时。
- 尝试 **清除 Cookie** 或 **切换账号** (参考本文档开头的账号管理章节)。
- 下次运行时降低频率 (增加随机延迟，减少每日总量)。

### 5. 文件保存报错 (Permission denied)
**现象**: `Permission denied: ... .xlsx`
**原因**: 目标文件（如 Excel）正在被其他软件打开。
**解决**: 关闭所有正在查看数据文件的程序（Excel/WPS），然后重试。
A: 当前版本每次运行只能选择一种格式，如需多种格式，请多次运行。
